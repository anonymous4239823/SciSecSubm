{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_directory = '../data/metadata'\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygad\n",
    "import copy\n",
    "import src\n",
    "\n",
    "class FlowOptimizer:\n",
    "    def __init__(self, packets, model_name, flow_id, max_fitness):\n",
    "        self.packets = packets\n",
    "        self.model_name = model_name\n",
    "        self.flow_id = flow_id\n",
    "        self.ga_instance = None\n",
    "        self.max_fitness = max_fitness\n",
    "        \n",
    "    def apply_best_solution(self):\n",
    "        best_solution, best_solution_fitness, _ = self.ga_instance.best_solution()\n",
    "        best_operations = self.decode_operations(best_solution)\n",
    "        # Stosujemy najlepsze operacje do pakietów\n",
    "        self.best_modified_packets = self.apply_operations_to_packets(best_operations, self.packets, self.flow_id)\n",
    "        # Możesz zwrócić więcej informacji o najlepszym rozwiązaniu, jeśli potrzebujesz\n",
    "        return self.best_modified_packets, best_solution_fitness\n",
    "\n",
    "    def on_generation(self, ga_instance):\n",
    "        #current_population = ga_instance.population\n",
    "        #print(f\"Flow ID: {self.flow_id}, Pokolenie: {ga_instance.generations_completed}\")\n",
    "        #print(\"Aktualna populacja:\")\n",
    "        #for solution in current_population:\n",
    "        #    print(solution)\n",
    "        # Tworzymy wiadomość zawierającą interesujące nas informacje\n",
    "        #message = f\"Najlepszy wynik: {ga_instance.best_solution()[1]}\\n\"\n",
    "        #print(message)  # Wyświetlamy informacje na konsoli\n",
    "        \n",
    "        # Zapisujemy te same informacje do pliku\n",
    "        #with open(f\"progress_for_flow_{self.flow_id}.txt\", \"a\") as file:\n",
    "        #    file.write(message)\n",
    "        pass\n",
    "            \n",
    "    def on_fitness(self, ga_instance):\n",
    "        ga_instance.plot_fitness()\n",
    "\n",
    "    def fitness_function(self, ga_instance, solution, solution_idx):\n",
    "        \n",
    "        fitnesses = []\n",
    "        target_fitnesses = []\n",
    "        \n",
    "        operations = self.decode_operations(solution)\n",
    "        \n",
    "        modified_packets = self.packets # always check nop\n",
    "        sizing_stats = prepare_size_stats(modified_packets, self.flow_id)\n",
    "        timing_stats = prepare_timing_stats(modified_packets, self.flow_id)\n",
    "        fitnesses.append(1.0 - predict_single_flow(self.model_name, sizing_stats, timing_stats))\n",
    "        target_fitnesses.append(1.0 - predict_single_flow_target(self.model_name, sizing_stats, timing_stats))\n",
    "        \n",
    "        modified_packets = self.apply_operations_to_packets(operations[0], op_type = 'size_norm')\n",
    "        sizing_stats = prepare_size_stats(modified_packets, self.flow_id)\n",
    "        #print(sizing_stats)\n",
    "        timing_stats = prepare_timing_stats(modified_packets, self.flow_id)\n",
    "        fitnesses.append(1.0 - predict_single_flow(self.model_name, sizing_stats, timing_stats))\n",
    "        target_fitnesses.append(1.0 - predict_single_flow_target(self.model_name, sizing_stats, timing_stats))\n",
    "        \n",
    "        modified_packets = self.apply_operations_to_packets(operations[1], op_type = 'size_uni')\n",
    "        sizing_stats = prepare_size_stats(modified_packets, self.flow_id)\n",
    "        timing_stats = prepare_timing_stats(modified_packets, self.flow_id)\n",
    "        fitnesses.append(1.0 - predict_single_flow(self.model_name, sizing_stats, timing_stats))\n",
    "        target_fitnesses.append(1.0 - predict_single_flow_target(self.model_name, sizing_stats, timing_stats))\n",
    "        \n",
    "        modified_packets = self.apply_operations_to_packets(operations[2], op_type = 'time_norm')\n",
    "        sizing_stats = prepare_size_stats(modified_packets, self.flow_id)\n",
    "        timing_stats = prepare_timing_stats(modified_packets, self.flow_id)\n",
    "        fitnesses.append(1.0 - predict_single_flow(self.model_name, sizing_stats, timing_stats))\n",
    "        target_fitnesses.append(1.0 - predict_single_flow_target(self.model_name, sizing_stats, timing_stats))\n",
    "        \n",
    "        modified_packets = self.apply_operations_to_packets(operations[3], op_type = 'time_uni')\n",
    "        sizing_stats = prepare_size_stats(modified_packets, self.flow_id)\n",
    "        timing_stats = prepare_timing_stats(modified_packets, self.flow_id)\n",
    "        fitnesses.append(1.0 - predict_single_flow(self.model_name, sizing_stats, timing_stats))\n",
    "        target_fitnesses.append(1.0 - predict_single_flow_target(self.model_name, sizing_stats, timing_stats))\n",
    "        \n",
    "        #print(fitnesses)\n",
    "        max_fitness_value = max(fitnesses)\n",
    "        max_fitness_index = fitnesses.index(max_fitness_value)\n",
    "        #print(max_fitness_index)\n",
    "\n",
    "        max_fitness_target = target_fitnesses[max_fitness_index]\n",
    "\n",
    "        \n",
    "        # Zapisz maksymalną wartość fitness i indeks, jeśli jest większa niż obecnie zapisana maksymalna wartość\n",
    "        #print(self.max_fitness)\n",
    "        if max_fitness_value > self.max_fitness[0][0]:\n",
    "            #print(f'CONDITION max: {max_fitness_value}, prev: {self.max_fitness[0]}')\n",
    "            self.max_fitness = [[max_fitness_value, max_fitness_target], self.decode_operations(solution), max_fitness_index]\n",
    "            #print(f'self:{self.max_fitness[0]}')\n",
    "        \n",
    "        #print(max_fitness_value)\n",
    "        #print(f'solution:{solution}')\n",
    "        #print(f'finresses: {fitnesses}, solution:{self.decode_operations(solution)}')\n",
    "        \n",
    "        return max_fitness_value\n",
    "\n",
    "    def optimize_for_flow(self):\n",
    "        \n",
    "        #with open(f\"progress_for_flow_{self.flow_id}.txt\", \"w\") as file:\n",
    "        #    file.write(\"\")  # Tylko czyszczenie zawartości pliku\n",
    "            \n",
    "        ga_instance = pygad.GA(num_generations=2,\n",
    "                               num_parents_mating=1,\n",
    "                               fitness_func=self.fitness_function,\n",
    "                               sol_per_pop=3,                          \n",
    "                               mutation_probability=0.7,\n",
    "                               suppress_warnings= True,\n",
    "     #                          initial_population=np.array([\n",
    "    #        [4.14645349, 9.76164109, 1.67182642, 2.09448623, 1, 0],\n",
    "   #         [2.80090477, 4.24612439, 1.41426283, 1.12520895, 2, 1],\n",
    "  #          [1.31962389, 1.47082939, 2.15440462, 1.3226799, 0, 2]\n",
    " #       ]),\n",
    "                               on_generation=self.on_generation,\n",
    "                               num_genes=6,  # Przykład, dostosuj do ilości operacji\n",
    "                               gene_space = [{'low': 1.1, 'high': 5.0},  # size normal\n",
    "                                             {'low': 0.25, 'high': 5.0}, # packet size uniform\n",
    "                                             {'low': 1.1, 'high': 3.0}, # time normal\n",
    "                                             {'low': 0.4, 'high': 2.4}, # time uniform\n",
    "                                            [0,1,2],  # direction\n",
    "                                            [0,1,2]]) # focus point \n",
    "\n",
    "        \n",
    "        ga_instance.run()\n",
    "        return ga_instance, self.max_fitness\n",
    "\n",
    "    def decode_operations(self, genotype):\n",
    "        operations = []\n",
    "        \n",
    "\n",
    "        focus_point_map = {0: \"start\", 1: \"middle\", 2: \"end\"}\n",
    "        focus_point = None\n",
    "        focus_point_index = genotype[5] \n",
    "        focus_point = focus_point_map.get(focus_point_index, None)  \n",
    "\n",
    "        size_norm = {\n",
    "            'scaling_factor': genotype[0],\n",
    "            'direction': genotype[4],\n",
    "            'focus_point': focus_point\n",
    "        }\n",
    "        \n",
    "        size_uni = {\n",
    "            'scaling_factor': genotype[1],\n",
    "            'direction': genotype[4],\n",
    "            'focus_point': focus_point\n",
    "        }\n",
    "        \n",
    "        time_norm = {\n",
    "            'scaling_factor': genotype[2],\n",
    "            'direction': genotype[4],\n",
    "            'focus_point': focus_point\n",
    "        }\n",
    "        \n",
    "        \n",
    "        time_uni = {\n",
    "            'scaling_factor': genotype[3],\n",
    "            'direction': genotype[4],\n",
    "            'focus_point': focus_point\n",
    "        }  \n",
    "            \n",
    "            \n",
    "        \n",
    "        operations = [size_norm, size_uni, time_norm, time_uni]\n",
    "        return operations \n",
    "        \n",
    "\n",
    "\n",
    "    def apply_operations_to_packets(self, operation, op_type):\n",
    "\n",
    "        #packets = copy.deepcopy(self.packets) # deep copy for testing\n",
    "        #print(f'size: {prepare_size_stats(self.packets, self.flow_id)}')\n",
    "        #print(f'time: {prepare_timing_stats(self.packets, self.flow_id)}')\n",
    "\n",
    "\n",
    "        scaling_factor = operation['scaling_factor']\n",
    "        direction = operation.get('direction')\n",
    "        focus_point = operation.get('focus_point')\n",
    "        \n",
    "\n",
    "        if op_type == 'size_norm':\n",
    "            packets = adjust_packet_size_deepcopy(self.packets, self.flow_id, direction, 'normal', scaling_factor, focus_point)\n",
    "            \n",
    "            \n",
    "        elif op_type == 'size_uni':\n",
    "            packets = adjust_packet_size_deepcopy(self.packets, self.flow_id, direction, 'uniform', scaling_factor, focus_point)\n",
    "            \n",
    "        elif op_type == 'time_norm':\n",
    "            packets = apply_time_perturbation_with_focus_deepcopy(self.packets, self.flow_id, 'normal', focus_point, scaling_factor)\n",
    "            \n",
    "        elif op_type == 'time_uni':\n",
    "            #print(f'przed: {prepare_timing_stats(self.packets, self.flow_id)}')\n",
    "            packets = apply_time_perturbation_with_focus_deepcopy(self.packets, self.flow_id, 'uniform', focus_point, scaling_factor)\n",
    "            #print(f'po: {prepare_timing_stats(packets, self.flow_id)}')\n",
    "            \n",
    "        #print(packets)\n",
    "        return packets\n",
    "    \n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "    #print(optimizer.best_solution())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to process: ga_rf_fitness_botnet-capture-20110815-rbot-dos_perturbed.pkl\n",
      "Error loading ../data/metadata/ga_rf_fitness_botnet-capture-20110815-rbot-dos_perturbed.pkl: Can't get attribute 'FlowOptimizer' on <module '__main__'>\n",
      "Skipping file due to loading error: ga_rf_fitness_botnet-capture-20110815-rbot-dos_perturbed.pkl\n",
      "Attempting to process: ga_rf_fitness_botnet-capture-20110815-fast-flux-2_perturbed.pkl\n",
      "Error loading ../data/metadata/ga_rf_fitness_botnet-capture-20110815-fast-flux-2_perturbed.pkl: Can't get attribute 'FlowOptimizer' on <module '__main__'>\n",
      "Skipping file due to loading error: ga_rf_fitness_botnet-capture-20110815-fast-flux-2_perturbed.pkl\n",
      "Attempting to process: ga_rf_fitness_botnet-capture-20110812-rbot_perturbed.pkl\n",
      "Error loading ../data/metadata/ga_rf_fitness_botnet-capture-20110812-rbot_perturbed.pkl: Can't get attribute 'FlowOptimizer' on <module '__main__'>\n",
      "Skipping file due to loading error: ga_rf_fitness_botnet-capture-20110812-rbot_perturbed.pkl\n",
      "Attempting to process: ga_rf_fitness_botnet-capture-20110816-sogou_perturbed.pkl\n",
      "Error loading ../data/metadata/ga_rf_fitness_botnet-capture-20110816-sogou_perturbed.pkl: Can't get attribute 'FlowOptimizer' on <module '__main__'>\n",
      "Skipping file due to loading error: ga_rf_fitness_botnet-capture-20110816-sogou_perturbed.pkl\n",
      "Attempting to process: ga_rf_fitness_botnet-capture-20110816-donbot_perturbed.pkl\n",
      "Error loading ../data/metadata/ga_rf_fitness_botnet-capture-20110816-donbot_perturbed.pkl: Can't get attribute 'FlowOptimizer' on <module '__main__'>\n",
      "Skipping file due to loading error: ga_rf_fitness_botnet-capture-20110816-donbot_perturbed.pkl\n",
      "Attempting to process: ga_rf_fitness_botnet-capture-20110815-fast-flux_perturbed.pkl\n",
      "Error loading ../data/metadata/ga_rf_fitness_botnet-capture-20110815-fast-flux_perturbed.pkl: Can't get attribute 'FlowOptimizer' on <module '__main__'>\n",
      "Skipping file due to loading error: ga_rf_fitness_botnet-capture-20110815-fast-flux_perturbed.pkl\n",
      "Total files processed: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def load_object_pickle(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'rb') as pickle_file:\n",
    "            obj = pickle.load(pickle_file)\n",
    "        return obj\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_and_save_results(directory_path):\n",
    "    files_processed = 0\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if \"20110817-bot\" not in filename and \"20110819-bot\" not in filename and '20110810-neris' not in filename:\n",
    "            if filename.endswith(\".pkl\"):\n",
    "                filepath = os.path.join(directory_path, filename)\n",
    "                print(f\"Attempting to process: {filename}\")\n",
    "                obj = load_object_pickle(filepath)\n",
    "                if obj is not None:\n",
    "                    try:\n",
    "                        for i in range(len(obj)):\n",
    "                            list = obj[i][0][1][0]\n",
    "                            file_key = filename.replace(\".pkl\", \"\").replace(\"_perturbed\", \"\")\n",
    "                            \n",
    "                            # Nazwa pliku wyjściowego odpowiadająca file_key\n",
    "                            output_filepath = f\"{file_key}.txt\"\n",
    "                            \n",
    "                            # Otwieranie pliku do zapisu wyników dla każdego pliku .pkl\n",
    "                            with open(output_filepath, 'w') as output_file:\n",
    "                                results_line = f\"{file_key}: {list}\\n\"\n",
    "                                output_file.write(results_line)\n",
    "                                print(f\"Successfully processed and saved: {file_key}\")\n",
    "                                files_processed += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing data from {filename}: {e}\")\n",
    "                else:\n",
    "                    print(f\"Skipping file due to loading error: {filename}\")\n",
    "    print(f\"Total files processed: {files_processed}\")\n",
    "\n",
    "# Przykład użycia\n",
    "#directory_path = '../../../../../../Volumes/Extreme_SSD/data/metadata'  # Ścieżka do twojego katalogu\n",
    "directory_path = '../data/metadata' \n",
    "process_and_save_results(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to process: ga_rf_fitness_botnet-capture-20110815-rbot-dos_perturbed.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: No IPv4 address found on anpi1 !\n",
      "WARNING: No IPv4 address found on anpi0 !\n",
      "WARNING: more No IPv4 address found on en3 !\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved: ga_rf_fitness_botnet-capture-20110815-rbot-dos\n",
      "Attempting to process: ga_rf_fitness_botnet-capture-20110815-fast-flux-2_perturbed.pkl\n",
      "Successfully processed and saved: ga_rf_fitness_botnet-capture-20110815-fast-flux-2\n",
      "Attempting to process: ga_rf_fitness_botnet-capture-20110812-rbot_perturbed.pkl\n",
      "Successfully processed and saved: ga_rf_fitness_botnet-capture-20110812-rbot\n",
      "Attempting to process: ga_rf_fitness_botnet-capture-20110816-sogou_perturbed.pkl\n",
      "Successfully processed and saved: ga_rf_fitness_botnet-capture-20110816-sogou\n",
      "Attempting to process: ga_rf_fitness_botnet-capture-20110816-donbot_perturbed.pkl\n",
      "Successfully processed and saved: ga_rf_fitness_botnet-capture-20110816-donbot\n",
      "Attempting to process: ga_rf_fitness_botnet-capture-20110816-qvod_perturbed.pkl\n",
      "Successfully processed and saved: ga_rf_fitness_botnet-capture-20110816-qvod\n",
      "Attempting to process: ga_rf_fitness_botnet-capture-20110815-fast-flux_perturbed.pkl\n",
      "Successfully processed and saved: ga_rf_fitness_botnet-capture-20110815-fast-flux\n",
      "Total files processed: 7\n"
     ]
    }
   ],
   "source": [
    "directory_path = '../data/metadata'  # Ścieżka do twojego katalogu\n",
    "output_filepath = 'processed_results.txt'  # Plik wyjściowy z wynikami\n",
    "process_and_save_results(directory_path, output_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "def load_object_pickle(filepath):\n",
    "    \"\"\"\n",
    "    Loads an object from a specified filepath using dill.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'rb') as pickle_file:\n",
    "        obj = pickle.load(pickle_file)\n",
    "    return obj\n",
    "\n",
    "\n",
    "# Funkcja do przetwarzania plików w katalogu\n",
    "def process_pickle_files(directory_path):\n",
    "    results = {}\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        print(filename)\n",
    "        if filename.endswith(\".pkl\"):\n",
    "            filepath = os.path.join(directory_path, filename)\n",
    "            obj = load_object_pickle(filepath)\n",
    "            # Zakładamy, że obj[1] to nasza tablica/tablice do ekstrakcji\n",
    "            list1 = obj[1][0] if isinstance(obj[1][0], list) else [obj[1][0]]\n",
    "            list2 = obj[1][1:] if len(obj[1]) > 1 else []\n",
    "\n",
    "            file_key = filename.replace(\".pki\", \"\").replace(\"_perturbed\", \"\")\n",
    "            results[file_key] = [list1, list2]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyberaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
