{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved files testing\n",
    "from src.utils.flow import *\n",
    "from src.utils.flow_calculations import *\n",
    "from src.utils.restoration import *\n",
    "from src.utils.truncated_packet import *\n",
    "from src.operations.size_perturbation_logic import *\n",
    "from src.operations.timing_perturbation_logic import *\n",
    "from src.operations.calculate_fitness import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_best_on_packets(packets, max_fitness, flow_id):\n",
    "\n",
    "    index = max_fitness[1][2]\n",
    "    #print(index)\n",
    "    if index == 0:\n",
    "        return packets\n",
    "    \n",
    "    params = max_fitness[1][1][index-1] # best technique\n",
    "    #print(f'saved:{params}')\n",
    "    \n",
    "    if index == 1:\n",
    "        #print('packet_size_nor')\n",
    "        #print(f'size: {prepare_size_stats(packets, flow_id)}')\n",
    "        \n",
    "        return adjust_packet_size(packets, flow_id, params['direction'], 'normal', params['scaling_factor'], params['focus_point'])\n",
    "        \n",
    "                \n",
    "    elif index == 2:\n",
    "        #print('packet_size_uni')\n",
    "        #print(f'size: {prepare_size_stats(packets, flow_id)}')\n",
    "        return adjust_packet_size(packets, flow_id, params['direction'], 'uniform', params['scaling_factor'], params['focus_point'])\n",
    "                \n",
    "    elif index == 3:\n",
    "        #print('packet_time_norm')\n",
    "        #print(f'time: {prepare_timing_stats(packets, flow_id)}')\n",
    "        return apply_time_perturbation_with_focus(packets, flow_id, 'normal', params['focus_point'], params['scaling_factor'])\n",
    "                \n",
    "    elif index == 4:\n",
    "        #print('packet_time_uni')\n",
    "        #print(f'time: {prepare_timing_stats(packets, flow_id)}')\n",
    "        return apply_time_perturbation_with_focus(packets, flow_id, 'uniform', params['focus_point'], params['scaling_factor'])\n",
    "        \n",
    "    return packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygad\n",
    "import copy\n",
    "\n",
    "class FlowOptimizer:\n",
    "    def __init__(self, packets, model_name, flow_id, max_fitness):\n",
    "        self.packets = packets\n",
    "        self.model_name = model_name\n",
    "        self.flow_id = flow_id\n",
    "        self.ga_instance = None\n",
    "        self.max_fitness = max_fitness\n",
    "        \n",
    "    def apply_best_solution(self):\n",
    "        best_solution, best_solution_fitness, _ = self.ga_instance.best_solution()\n",
    "        best_operations = self.decode_operations(best_solution)\n",
    "        # Stosujemy najlepsze operacje do pakietów\n",
    "        self.best_modified_packets = self.apply_operations_to_packets(best_operations, self.packets, self.flow_id)\n",
    "        # Możesz zwrócić więcej informacji o najlepszym rozwiązaniu, jeśli potrzebujesz\n",
    "        return self.best_modified_packets, best_solution_fitness\n",
    "\n",
    "    def on_generation(self, ga_instance):\n",
    "        #current_population = ga_instance.population\n",
    "        #print(f\"Flow ID: {self.flow_id}, Pokolenie: {ga_instance.generations_completed}\")\n",
    "        #print(\"Aktualna populacja:\")\n",
    "        #for solution in current_population:\n",
    "        #    print(solution)\n",
    "        # Tworzymy wiadomość zawierającą interesujące nas informacje\n",
    "        #message = f\"Najlepszy wynik: {ga_instance.best_solution()[1]}\\n\"\n",
    "        #print(message)  # Wyświetlamy informacje na konsoli\n",
    "        \n",
    "        # Zapisujemy te same informacje do pliku\n",
    "        #with open(f\"progress_for_flow_{self.flow_id}.txt\", \"a\") as file:\n",
    "        #    file.write(message)\n",
    "        pass\n",
    "            \n",
    "    def on_fitness(self, ga_instance):\n",
    "        ga_instance.plot_fitness()\n",
    "\n",
    "    def fitness_function(self, ga_instance, solution, solution_idx):\n",
    "        \n",
    "        fitnesses = []\n",
    "        target_fitnesses = []\n",
    "        \n",
    "        operations = self.decode_operations(solution)\n",
    "        \n",
    "        modified_packets = self.packets # always check nop\n",
    "        sizing_stats = prepare_size_stats(modified_packets, self.flow_id)\n",
    "        timing_stats = prepare_timing_stats(modified_packets, self.flow_id)\n",
    "        fitnesses.append(1.0 - predict_single_flow(self.model_name, sizing_stats, timing_stats))\n",
    "        target_fitnesses.append(1.0 - predict_single_flow_target(self.model_name, sizing_stats, timing_stats))\n",
    "        \n",
    "        modified_packets = self.apply_operations_to_packets(operations[0], op_type = 'size_norm')\n",
    "        sizing_stats = prepare_size_stats(modified_packets, self.flow_id)\n",
    "        #print(sizing_stats)\n",
    "        timing_stats = prepare_timing_stats(modified_packets, self.flow_id)\n",
    "        fitnesses.append(1.0 - predict_single_flow(self.model_name, sizing_stats, timing_stats))\n",
    "        target_fitnesses.append(1.0 - predict_single_flow_target(self.model_name, sizing_stats, timing_stats))\n",
    "        \n",
    "        modified_packets = self.apply_operations_to_packets(operations[1], op_type = 'size_uni')\n",
    "        sizing_stats = prepare_size_stats(modified_packets, self.flow_id)\n",
    "        timing_stats = prepare_timing_stats(modified_packets, self.flow_id)\n",
    "        fitnesses.append(1.0 - predict_single_flow(self.model_name, sizing_stats, timing_stats))\n",
    "        target_fitnesses.append(1.0 - predict_single_flow_target(self.model_name, sizing_stats, timing_stats))\n",
    "        \n",
    "        modified_packets = self.apply_operations_to_packets(operations[2], op_type = 'time_norm')\n",
    "        sizing_stats = prepare_size_stats(modified_packets, self.flow_id)\n",
    "        timing_stats = prepare_timing_stats(modified_packets, self.flow_id)\n",
    "        fitnesses.append(1.0 - predict_single_flow(self.model_name, sizing_stats, timing_stats))\n",
    "        target_fitnesses.append(1.0 - predict_single_flow_target(self.model_name, sizing_stats, timing_stats))\n",
    "        \n",
    "        modified_packets = self.apply_operations_to_packets(operations[3], op_type = 'time_uni')\n",
    "        sizing_stats = prepare_size_stats(modified_packets, self.flow_id)\n",
    "        timing_stats = prepare_timing_stats(modified_packets, self.flow_id)\n",
    "        fitnesses.append(1.0 - predict_single_flow(self.model_name, sizing_stats, timing_stats))\n",
    "        target_fitnesses.append(1.0 - predict_single_flow_target(self.model_name, sizing_stats, timing_stats))\n",
    "        \n",
    "        #print(fitnesses)\n",
    "        max_fitness_value = max(fitnesses)\n",
    "        max_fitness_index = fitnesses.index(max_fitness_value)\n",
    "        #print(max_fitness_index)\n",
    "\n",
    "        max_fitness_target = target_fitnesses[max_fitness_index]\n",
    "\n",
    "        \n",
    "        # Zapisz maksymalną wartość fitness i indeks, jeśli jest większa niż obecnie zapisana maksymalna wartość\n",
    "        #print(self.max_fitness)\n",
    "        if max_fitness_value > self.max_fitness[0][0]:\n",
    "            #print(f'CONDITION max: {max_fitness_value}, prev: {self.max_fitness[0]}')\n",
    "            self.max_fitness = [[max_fitness_value, max_fitness_target], self.decode_operations(solution), max_fitness_index]\n",
    "            #print(f'self:{self.max_fitness[0]}')\n",
    "        \n",
    "        #print(max_fitness_value)\n",
    "        #print(f'solution:{solution}')\n",
    "        #print(f'finresses: {fitnesses}, solution:{self.decode_operations(solution)}')\n",
    "        \n",
    "        return max_fitness_value\n",
    "\n",
    "    def optimize_for_flow(self):\n",
    "        \n",
    "        #with open(f\"progress_for_flow_{self.flow_id}.txt\", \"w\") as file:\n",
    "        #    file.write(\"\")  # Tylko czyszczenie zawartości pliku\n",
    "            \n",
    "        ga_instance = pygad.GA(num_generations=2,\n",
    "                               num_parents_mating=1,\n",
    "                               fitness_func=self.fitness_function,\n",
    "                               sol_per_pop=3,                          \n",
    "                               mutation_probability=0.7,\n",
    "                               suppress_warnings= True,\n",
    "                               on_generation=self.on_generation,\n",
    "                               num_genes=6,  # Przykład, dostosuj do ilości operacji\n",
    "                               gene_space = [{'low': 1.1, 'high': 5.0},  # size normal\n",
    "                                             {'low': 0.25, 'high': 5.0}, # packet size uniform\n",
    "                                             {'low': 1.1, 'high': 3.0}, # time normal\n",
    "                                             {'low': 0.4, 'high': 2.4}, # time uniform\n",
    "                                            [0,1,2],  # direction\n",
    "                                            [0,1,2]]) # focus point \n",
    "\n",
    "        \n",
    "        ga_instance.run()\n",
    "        return ga_instance, self.max_fitness\n",
    "\n",
    "    def decode_operations(self, genotype):\n",
    "        operations = []\n",
    "        \n",
    "\n",
    "        focus_point_map = {0: \"start\", 1: \"middle\", 2: \"end\"}\n",
    "        focus_point = None\n",
    "        focus_point_index = genotype[5] \n",
    "        focus_point = focus_point_map.get(focus_point_index, None)  \n",
    "\n",
    "        size_norm = {\n",
    "            'scaling_factor': genotype[0],\n",
    "            'direction': genotype[4],\n",
    "            'focus_point': focus_point\n",
    "        }\n",
    "        \n",
    "        size_uni = {\n",
    "            'scaling_factor': genotype[1],\n",
    "            'direction': genotype[4],\n",
    "            'focus_point': focus_point\n",
    "        }\n",
    "        \n",
    "        time_norm = {\n",
    "            'scaling_factor': genotype[2],\n",
    "            'direction': genotype[4],\n",
    "            'focus_point': focus_point\n",
    "        }\n",
    "        \n",
    "        \n",
    "        time_uni = {\n",
    "            'scaling_factor': genotype[3],\n",
    "            'direction': genotype[4],\n",
    "            'focus_point': focus_point\n",
    "        }  \n",
    "            \n",
    "            \n",
    "        \n",
    "        operations = [size_norm, size_uni, time_norm, time_uni]\n",
    "        return operations \n",
    "        \n",
    "\n",
    "\n",
    "    def apply_operations_to_packets(self, operation, op_type):\n",
    "\n",
    "        #packets = copy.deepcopy(self.packets) # deep copy for testing\n",
    "        #print(f'size: {prepare_size_stats(self.packets, self.flow_id)}')\n",
    "        #print(f'time: {prepare_timing_stats(self.packets, self.flow_id)}')\n",
    "\n",
    "\n",
    "        scaling_factor = operation['scaling_factor']\n",
    "        direction = operation.get('direction')\n",
    "        focus_point = operation.get('focus_point')\n",
    "        \n",
    "\n",
    "        if op_type == 'size_norm':\n",
    "            packets = adjust_packet_size_deepcopy(self.packets, self.flow_id, direction, 'normal', scaling_factor, focus_point)\n",
    "            \n",
    "            \n",
    "        elif op_type == 'size_uni':\n",
    "            packets = adjust_packet_size_deepcopy(self.packets, self.flow_id, direction, 'uniform', scaling_factor, focus_point)\n",
    "            \n",
    "        elif op_type == 'time_norm':\n",
    "            packets = apply_time_perturbation_with_focus_deepcopy(self.packets, self.flow_id, 'normal', focus_point, scaling_factor)\n",
    "            \n",
    "        elif op_type == 'time_uni':\n",
    "            #print(f'przed: {prepare_timing_stats(self.packets, self.flow_id)}')\n",
    "            packets = apply_time_perturbation_with_focus_deepcopy(self.packets, self.flow_id, 'uniform', focus_point, scaling_factor)\n",
    "            #print(f'po: {prepare_timing_stats(packets, self.flow_id)}')\n",
    "            \n",
    "        #print(packets)\n",
    "        return packets\n",
    "    \n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "    #print(optimizer.best_solution())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_object_pickle(obj, filepath):\n",
    "    \"\"\"\n",
    "    Saves an object to a specified filepath using pickle.\n",
    "    \n",
    "    Parameters:\n",
    "    - obj: The object to save.\n",
    "    - filepath: The path to the file where the object should be saved.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    \n",
    "    with open(filepath, 'wb') as pickle_file:  # Note 'wb' for writing in binary mode\n",
    "        pickle.dump(obj, pickle_file)\n",
    "\n",
    "def load_object_pickle(filepath):\n",
    "    \"\"\"\n",
    "    Loads an object from a specified filepath using pickle.\n",
    "    \n",
    "    Parameters:\n",
    "    - filepath: The path to the file from which to load the object.\n",
    "    \n",
    "    Returns:\n",
    "    - The loaded object.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'rb') as pickle_file:  # Note 'rb' for reading in binary mode\n",
    "        obj = pickle.load(pickle_file)\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "captures = [\n",
    "    \"botnet-capture-20110811-neris\",\n",
    "    \"botnet-capture-20110812-rbot\",\n",
    "    \"botnet-capture-20110815-fast-flux-2\",\n",
    "    \"botnet-capture-20110815-fast-flux\",\n",
    "    \"botnet-capture-20110815-rbot-dos\",\n",
    "    \"botnet-capture-20110816-donbot\",\n",
    "    \"botnet-capture-20110816-qvod\",\n",
    "    \"botnet-capture-20110816-sogou\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on capture: botnet-capture-20110811-neris\n",
      "Total packets loaded: 175491\n",
      "Process 2831: Starting work on flow_id: 1\n",
      "Process 2832: Starting work on flow_id: 2\n",
      "Process 2833: Starting work on flow_id: 3\n",
      "Process 2834: Starting work on flow_id: 4\n",
      "Process 2836: Starting work on flow_id: 6\n",
      "Process 2835: Starting work on flow_id: 5\n",
      "Process 2837: Starting work on flow_id: 7\n",
      "Process 2838: Starting work on flow_id: 8\n",
      "Process 2840: Starting work on flow_id: 10\n",
      "Process 2839: Starting work on flow_id: 9\n",
      "Process 2841: Starting work on flow_id: 11\n",
      "Process 2843: Starting work on flow_id: 13\n",
      "Process 2842: Starting work on flow_id: 12\n",
      "Process 2844: Starting work on flow_id: 14\n",
      "Process 2845: Starting work on flow_id: 15\n",
      "Process 2846: Starting work on flow_id: 16\n",
      "Process 2849: Starting work on flow_id: 19\n",
      "Process 2847: Starting work on flow_id: 17\n",
      "Process 2848: Starting work on flow_id: 18\n",
      "Process 2852: Starting work on flow_id: 22\n",
      "Process 2850: Starting work on flow_id: 20\n",
      "Process 2851: Starting work on flow_id: 21\n",
      "Process 2855: Starting work on flow_id: 25\n",
      "Process 2854: Starting work on flow_id: 24\n",
      "Process 2853: Starting work on flow_id: 23\n",
      "Process 2857: Starting work on flow_id: 27\n",
      "Process 2856: Starting work on flow_id: 26\n",
      "Process 2858: Starting work on flow_id: 28\n",
      "Process 2861: Starting work on flow_id: 31\n",
      "Process 2859: Starting work on flow_id: 29\n",
      "Process 2860: Starting work on flow_id: 30\n",
      "Process 2862: Starting work on flow_id: 32\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Process 2863: Starting work on flow_id: 33Process 2864: Starting work on flow_id: 34\n",
      "\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Process 2865: Starting work on flow_id: 35\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Not enough packets to apply perturbation.\n",
      "Process 2866: Starting work on flow_id: 36\n",
      "Process 2831: Starting work on flow_id: 39\n",
      "Process 2833: Starting work on flow_id: 40\n",
      "Process 2834: Starting work on flow_id: 42\n",
      "\n",
      "Process 2832: Starting work on flow_id: 41Process 2835: Starting work on flow_id: 43\n",
      "\n",
      "Process 2867: Starting work on flow_id: 37Process 2836: Starting work on flow_id: 44\n",
      "Process 2868: Starting work on flow_id: 38\n",
      "Process 2838: Starting work on flow_id: 45\n",
      "Process 2837: Starting work on flow_id: 46\n",
      "Process 2840: Starting work on flow_id: 48\n",
      "Process 2839: Starting work on flow_id: 47\n",
      "Process 2841: Starting work on flow_id: 49\n",
      "Process 2842: Starting work on flow_id: 50\n",
      "Process 2843: Starting work on flow_id: 51\n",
      "Process 2845: Starting work on flow_id: 52\n",
      "Process 2848: Starting work on flow_id: 53\n",
      "Process 2849: Starting work on flow_id: 54\n",
      "Process 2844: Starting work on flow_id: 55\n",
      "Process 2850: Starting work on flow_id: 56\n",
      "Process 2846: Starting work on flow_id: 57\n",
      "Process 2855: Starting work on flow_id: 59\n",
      "\n",
      "Process 2847: Starting work on flow_id: 58Process 2852: Starting work on flow_id: 60\n",
      "Process 2853: Starting work on flow_id: 61\n",
      "Process 2851: Starting work on flow_id: 62\n",
      "Process 2854: Starting work on flow_id: 63\n",
      "Process 2858: Starting work on flow_id: 64\n",
      "Process 2856: Starting work on flow_id: 66\n",
      "Process 2857: Starting work on flow_id: 65\n",
      "Process 2861: Starting work on flow_id: 67\n",
      "Process 2860: Starting work on flow_id: 68\n",
      "Process 2859: Starting work on flow_id: 69\n",
      "Process 2864: Starting work on flow_id: 70\n",
      "Process 2865: Starting work on flow_id: 71\n",
      "Process 2863: Starting work on flow_id: 72\n",
      "Process 2862: Starting work on flow_id: 73\n",
      "Process 2866: Starting work on flow_id: 74\n",
      "Process 2867: Starting work on flow_id: 75\n",
      "Process 2868: Starting work on flow_id: 76\n",
      "Process 2831: Starting work on flow_id: 77\n",
      "Process 2833: Starting work on flow_id: 78\n",
      "Process 2835: Starting work on flow_id: 79\n",
      "Process 2834: Starting work on flow_id: 80\n",
      "Process 2836: Starting work on flow_id: 81\n",
      "Process 2832: Starting work on flow_id: 82\n",
      "Process 2837: Starting work on flow_id: 83\n",
      "Process 2838: Starting work on flow_id: 84\n",
      "Process 2840: Starting work on flow_id: 85\n",
      "Process 2841: Starting work on flow_id: 86\n",
      "Process 2839: Starting work on flow_id: 87\n",
      "Process 2842: Starting work on flow_id: 88\n",
      "Process 2843: Starting work on flow_id: 89\n",
      "Process 2855: Starting work on flow_id: 90\n",
      "Process 2849: Starting work on flow_id: 91\n",
      "Process 2845: Starting work on flow_id: 92\n",
      "Process 2850: Starting work on flow_id: 93\n",
      "Process 2846: Starting work on flow_id: 94\n",
      "Process 2847: Starting work on flow_id: 95\n",
      "Process 2844: Starting work on flow_id: 96\n",
      "Process 2848: Starting work on flow_id: 97\n",
      "Process 2852: Starting work on flow_id: 98\n",
      "Process 2861: Starting work on flow_id: 99\n",
      "Process 2857: Starting work on flow_id: 100\n",
      "Process 2864: Starting work on flow_id: 101\n",
      "Process 2851: Starting work on flow_id: 102\n",
      "Process 2858: Starting work on flow_id: 103\n",
      "Process 2854: Starting work on flow_id: 104\n",
      "Process 2853: Starting work on flow_id: 105\n",
      "Process 2862: Starting work on flow_id: 106\n",
      "Process 2866: Starting work on flow_id: 107\n",
      "Process 2867: Starting work on flow_id: 108\n",
      "Process 2865: Starting work on flow_id: 109\n",
      "Process 2856: Starting work on flow_id: 110\n",
      "Process 2863: Starting work on flow_id: 111\n",
      "Process 2859: Starting work on flow_id: 112\n",
      "Process 2860: Starting work on flow_id: 113\n",
      "Process 2868: Starting work on flow_id: 114\n",
      "Process 2831: Starting work on flow_id: 115\n",
      "Process 2833: Starting work on flow_id: 116\n",
      "Process 2836: Starting work on flow_id: 117\n",
      "Process 2835: Starting work on flow_id: 118\n",
      "Process 2834: Starting work on flow_id: 119\n",
      "Process 2832: Starting work on flow_id: 120\n",
      "Process 2837: Starting work on flow_id: 121\n",
      "Process 2838: Starting work on flow_id: 122\n",
      "Process 2840: Starting work on flow_id: 123\n",
      "Process 2841: Starting work on flow_id: 124\n",
      "Process 2839: Starting work on flow_id: 125\n",
      "Process 2842: Starting work on flow_id: 126\n",
      "Process 2843: Starting work on flow_id: 127\n",
      "Process 2852: Starting work on flow_id: 128\n",
      "Process 2845: Starting work on flow_id: 129\n",
      "Process 2855: Starting work on flow_id: 130\n",
      "Process 2849: Starting work on flow_id: 131\n",
      "Process 2847: Starting work on flow_id: 132\n",
      "Process 2850: Starting work on flow_id: 133\n",
      "Process 2846: Starting work on flow_id: 134\n",
      "Process 2857: Starting work on flow_id: 135\n",
      "Process 2861: Starting work on flow_id: 136\n",
      "Process 2848: Starting work on flow_id: 137\n",
      "Process 2844: Starting work on flow_id: 138\n",
      "Process 2864: Starting work on flow_id: 139\n",
      "Process 2862: Starting work on flow_id: 140\n",
      "Process 2866: Starting work on flow_id: 141\n",
      "Process 2858: Starting work on flow_id: 142\n",
      "Process 2865: Starting work on flow_id: 143\n",
      "Process 2859: Starting work on flow_id: 144\n",
      "Process 2851: Starting work on flow_id: 145\n",
      "Process 2854: Starting work on flow_id: 146\n",
      "Process 2867: Starting work on flow_id: 147\n",
      "Process 2853: Starting work on flow_id: 148\n",
      "Process 2856: Starting work on flow_id: 149\n",
      "Process 2863: Starting work on flow_id: 150\n",
      "Process 2868: Starting work on flow_id: 151\n",
      "Process 2831: Starting work on flow_id: 152\n",
      "Process 2836: Starting work on flow_id: 153\n",
      "Process 2860: Starting work on flow_id: 154\n",
      "Process 2835: Starting work on flow_id: 155\n",
      "Process 2834: Starting work on flow_id: 156\n",
      "Process 2832: Starting work on flow_id: 157\n",
      "Process 2837: Starting work on flow_id: 158\n",
      "Process 2833: Starting work on flow_id: 159\n",
      "Process 2838: Starting work on flow_id: 160\n",
      "Process 2839: Starting work on flow_id: 161\n",
      "Process 2841: Starting work on flow_id: 162\n",
      "Process 2840: Starting work on flow_id: 163\n",
      "Process 2842: Starting work on flow_id: 164\n",
      "Process 2850: Starting work on flow_id: 165\n",
      "Process 2847: Starting work on flow_id: 166\n",
      "Process 2845: Starting work on flow_id: 167\n",
      "Process 2857: Starting work on flow_id: 168\n",
      "Process 2843: Starting work on flow_id: 169\n",
      "Process 2864: Starting work on flow_id: 170\n",
      "Process 2849: Starting work on flow_id: 171\n",
      "Process 2852: Starting work on flow_id: 172\n",
      "Process 2844: Starting work on flow_id: 173\n",
      "Process 2862: Starting work on flow_id: 174\n",
      "Process 2855: Starting work on flow_id: 175\n",
      "Process 2846: Starting work on flow_id: 176\n",
      "Process 2861: Starting work on flow_id: 177\n",
      "Process 2848: Starting work on flow_id: 178\n",
      "Process 2866: Starting work on flow_id: 179\n",
      "Process 2865: Starting work on flow_id: 180\n",
      "Process 2858: Starting work on flow_id: 181\n",
      "Process 2854: Starting work on flow_id: 182\n",
      "Process 2867: Starting work on flow_id: 183\n",
      "Process 2868: Starting work on flow_id: 184\n",
      "Process 2859: Starting work on flow_id: 185\n",
      "Process 2863: Starting work on flow_id: 186\n",
      "Process 2836: Starting work on flow_id: 187\n",
      "Process 2851: Starting work on flow_id: 188\n",
      "Process 2860: Starting work on flow_id: 189\n",
      "Process 2835: Starting work on flow_id: 190\n",
      "Process 2856: Starting work on flow_id: 191\n",
      "Process 2853: Starting work on flow_id: 192\n",
      "Process 2832: Starting work on flow_id: 193\n",
      "Process 2838: Starting work on flow_id: 194\n",
      "Process 2831: Starting work on flow_id: 195\n",
      "Process 2837: Starting work on flow_id: 196\n",
      "Process 2839: Starting work on flow_id: 197\n",
      "Process 2834: Starting work on flow_id: 198\n",
      "Process 2841: Starting work on flow_id: 199\n",
      "Process 2842: Starting work on flow_id: 200\n",
      "Process 2840: Starting work on flow_id: 201\n",
      "Process 2833: Starting work on flow_id: 202\n",
      "Process 2847: Starting work on flow_id: 203\n",
      "Process 2864: Starting work on flow_id: 204\n",
      "Process 2857: Starting work on flow_id: 205\n",
      "Process 2849: Starting work on flow_id: 206\n",
      "Process 2866: Starting work on flow_id: 207\n",
      "Process 2850: Starting work on flow_id: 208\n",
      "Process 2845: Starting work on flow_id: 209\n",
      "Process 2861: Starting work on flow_id: 210\n",
      "Process 2862: Starting work on flow_id: 211\n",
      "Process 2855: Starting work on flow_id: 212\n",
      "Process 2843: Starting work on flow_id: 213\n",
      "Process 2854: Starting work on flow_id: 214\n",
      "Process 2865: Starting work on flow_id: 215\n",
      "Process 2867: Starting work on flow_id: 216\n",
      "Process 2852: Starting work on flow_id: 217\n",
      "Process 2858: Starting work on flow_id: 218\n",
      "Process 2844: Starting work on flow_id: 219\n",
      "Process 2863: Starting work on flow_id: 220\n",
      "Process 2846: Starting work on flow_id: 221\n",
      "Process 2848: Starting work on flow_id: 222\n",
      "Process 2859: Starting work on flow_id: 223\n",
      "Process 2868: Starting work on flow_id: 224\n",
      "Process 2836: Starting work on flow_id: 225\n",
      "Process 2851: Starting work on flow_id: 226\n",
      "Process 2835: Starting work on flow_id: 227\n",
      "Process 2853: Starting work on flow_id: 228\n",
      "Process 2860: Starting work on flow_id: 229\n",
      "Process 2838: Starting work on flow_id: 230\n",
      "Process 2856: Starting work on flow_id: 231\n",
      "Process 2832: Starting work on flow_id: 232\n",
      "Process 2831: Starting work on flow_id: 233\n",
      "Process 2839: Starting work on flow_id: 234\n",
      "Process 2841: Starting work on flow_id: 235\n",
      "Process 2864: Starting work on flow_id: 236\n",
      "Process 2834: Starting work on flow_id: 237\n",
      "Process 2837: Starting work on flow_id: 238\n",
      "Process 2842: Starting work on flow_id: 239\n",
      "Process 2847: Starting work on flow_id: 240\n",
      "Process 2849: Starting work on flow_id: 241\n",
      "Process 2866: Starting work on flow_id: 242\n",
      "Process 2840: Starting work on flow_id: 243\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Załóżmy, że wszystkie funkcje takie jak create_truncated_packets_from_pcap, assign_flow_ids_to_packets, itp., są już zdefiniowane\n",
    "# Definicja klasy FlowOptimizer i inne potrzebne komponenty\n",
    "\n",
    "def process_flow_id(flow_id, packets, model):\n",
    "    print(f\"Process {os.getpid()}: Starting work on flow_id: {flow_id}\")\n",
    "    truncated_packets = copy.deepcopy(packets)  # Robimy głęboką kopię na poziomie procesu\n",
    "    inter = []\n",
    "    max_fitness = [0, [[0, 0], 0, 0], 0]\n",
    "    previous_max = 0\n",
    "    \n",
    "    for i in range(5):\n",
    "        optimizer = FlowOptimizer(truncated_packets, model, flow_id, max_fitness[1])\n",
    "        max_fitness = optimizer.optimize_for_flow()\n",
    "        inter.append(max_fitness[1][0][0])\n",
    "        \n",
    "        if max_fitness[1][0][0] > previous_max:\n",
    "            truncated_packets = apply_best_on_packets(truncated_packets, max_fitness, flow_id)\n",
    "            previous_max = max_fitness[1][0][0]\n",
    "\n",
    "    return flow_id, inter, truncated_packets\n",
    "\n",
    "def worker_job(flow_id, packets, model_name):\n",
    "    return process_flow_id(flow_id, packets, model_name)\n",
    "\n",
    "def main():\n",
    "    captures = [\n",
    "        \"botnet-capture-20110811-neris\"\n",
    "    ]\n",
    "\n",
    "    for capture in captures:\n",
    "        print(f'Working on capture: {capture}')\n",
    "\n",
    "        modified_pcap_path = f\"../data/interim/{capture}_perturbed.pcapng\"\n",
    "        pcap_file_path = f'../data/raw/{capture}.pcap'\n",
    "        save_object_path = f\"../data/metadata/ga_rf_fitness_{capture}_perturbed.pkl\"\n",
    "        model_name = f'{capture}'\n",
    "\n",
    "        truncated_packets = create_truncated_packets_from_pcap(pcap_file_path)\n",
    "        print(f\"Total packets loaded: {len(truncated_packets)}\")\n",
    "        truncated_packets = assign_flow_ids_to_packets(truncated_packets)\n",
    "\n",
    "        flow_ids = set(packet.flow_id for packet in truncated_packets)\n",
    "        extern = []\n",
    "\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=38) as executor:\n",
    "            futures = [executor.submit(worker_job, fid, truncated_packets, model_name) for fid in flow_ids]\n",
    "            results = [f.result() for f in concurrent.futures.as_completed(futures)]\n",
    "\n",
    "        for result in results:\n",
    "            flow_id, inter, updated_truncated_packets = result\n",
    "            extern.append(inter)\n",
    "            print(f\"Completed processing flow_id {flow_id}, final results: {inter}\")\n",
    "\n",
    "        modify_and_write_packets_one_by_one(pcap_file_path, modified_pcap_path, updated_truncated_packets)\n",
    "        with open(save_object_path, 'wb') as f:\n",
    "            pickle.dump(extern, f)\n",
    "\n",
    "        print(f\"Finished and saved: {capture}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # after PERTURBATIONS\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from src.data.cic_preprocess import *\n",
    "\n",
    "# Directory paths\n",
    "models_dir = \"../models\"\n",
    "data_dir = \"../data/processed\"\n",
    "\n",
    "# Loading models\n",
    "models = {file.split('_model.cbm')[0]: CatBoostRegressor().load_model(os.path.join(models_dir, file))\n",
    "          for file in os.listdir(models_dir) if file.endswith('.cbm')}\n",
    "\n",
    "for capture in captures:\n",
    "\n",
    "    # Testing models\n",
    "    accuracies = []\n",
    "    for model_name, model in models.items():\n",
    "        if capture in model_name and \"regressor\" in model_name:\n",
    "            # File name of the test data corresponds to the model name\n",
    "            test_file_name = f\"{capture}_orig_flows.csv\"\n",
    "            test_file_path = os.path.join(data_dir, test_file_name)\n",
    "            \n",
    "            # Loading test data\n",
    "            X_test = preprocess_traffic(test_file_path)\n",
    "            y_test = np.ones(X_test.shape[0])  # Assuming all samples are malicious, so y_test should be all ones\n",
    "            \n",
    "            # Model prediction\n",
    "            predictions = model.predict(X_test)\n",
    "            \n",
    "            # Calculating accuracy\n",
    "            accuracy = np.mean(predictions == y_test)\n",
    "            accuracies.append(predictions)\n",
    "            \n",
    "            predictions = pd.DataFrame(predictions).clip(lower=0, upper =1)\n",
    "            \n",
    "            \n",
    "            # Detailed logging\n",
    "            print(f\"Testing {model_name}:\")\n",
    "            print(f\"Accuracy: {accuracy:.5%}\")\n",
    "            print(f\"Predicted distribution: {predictions.mean()}\")\n",
    "            print(f\"Number of samples: {len(predictions)}\")\n",
    "            print(f\"Number of malicious (1) predictions: {np.sum(predictions == 1)}\")\n",
    "            print(f\"Number of benign (0) predictions: {np.sum(predictions == 0)}\\n\")\n",
    "\n",
    "            # Average accuracy\n",
    "            #mean_accuracy = np.mean(accuracies)\n",
    "            #print(f\"\\nAverage accuracy across all test cases: {mean_accuracy:.5%}\")\n",
    "            \n",
    "           \n",
    "            import matplotlib.pyplot as plt\n",
    "\n",
    "            # Assuming your DataFrame is named 'df' and the column with predictions is named 'Predictions'\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(predictions, bins=30, color='skyblue', edgecolor='black', log=True)\n",
    "            plt.title(f'Predictions Histogram (Log Scale) for {capture}_perturbed', fontsize=15)\n",
    "            plt.xlabel('Prediction Value', fontsize=12)\n",
    "            plt.ylabel('Frequency (Log Scale)', fontsize=12)\n",
    "            plt.grid(axis='y', alpha=0.75)\n",
    "\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # after PERTURBATIONS\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from src.data.cic_preprocess import *\n",
    "\n",
    "# Directory paths\n",
    "models_dir = \"../models\"\n",
    "data_dir = \"../data/processed\"\n",
    "\n",
    "# Loading models\n",
    "models = {file.split('_model.cbm')[0]: CatBoostRegressor().load_model(os.path.join(models_dir, file))\n",
    "          for file in os.listdir(models_dir) if file.endswith('.cbm')}\n",
    "\n",
    "for capture in captures:\n",
    "\n",
    "    # Testing models\n",
    "    accuracies = []\n",
    "    for model_name, model in models.items():\n",
    "        if capture in model_name and \"regressor\" in model_name:\n",
    "            # File name of the test data corresponds to the model name\n",
    "            test_file_name = f\"{capture}_perturbed_flows.csv\"\n",
    "            test_file_path = os.path.join(data_dir, test_file_name)\n",
    "            \n",
    "            # Loading test data\n",
    "            X_test = preprocess_traffic(test_file_path)\n",
    "            y_test = np.ones(X_test.shape[0])  # Assuming all samples are malicious, so y_test should be all ones\n",
    "            \n",
    "            # Model prediction\n",
    "            predictions = model.predict(X_test)\n",
    "            \n",
    "            # Calculating accuracy\n",
    "            #accuracy = np.mean(predictions == y_test)\n",
    "            #accuracies.append(predictions)\n",
    "            \n",
    "            predictions = pd.DataFrame(predictions).clip(lower=0, upper =1)\n",
    "            \n",
    "            \n",
    "            # Detailed logging\n",
    "            print(f\"Testing {model_name}:\")\n",
    "            print(f\"Accuracy: {accuracy:.5%}\")\n",
    "            print(f\"Predicted distribution: {predictions.mean()}\")\n",
    "            print(f\"Number of samples: {len(predictions)}\")\n",
    "            print(f\"Number of malicious (1) predictions: {np.sum(predictions == 1)}\")\n",
    "            print(f\"Number of benign (0) predictions: {np.sum(predictions == 0)}\\n\")\n",
    "\n",
    "            # Average accuracy\n",
    "            mean_accuracy = np.mean(accuracies)\n",
    "            print(f\"\\nAverage accuracy across all test cases: {mean_accuracy:.5%}\")\n",
    "            \n",
    "           \n",
    "            import matplotlib.pyplot as plt\n",
    "\n",
    "            # Assuming your DataFrame is named 'df' and the column with predictions is named 'Predictions'\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(predictions, bins=30, color='skyblue', edgecolor='black', log=True)\n",
    "            plt.title(f'Predictions Histogram (Log Scale) for {capture}_perturbed', fontsize=15)\n",
    "            plt.xlabel('Prediction Value', fontsize=12)\n",
    "            plt.ylabel('Frequency (Log Scale)', fontsize=12)\n",
    "            plt.grid(axis='y', alpha=0.75)\n",
    "\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
