{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved files testing\n",
    "from src.utils.flow import *\n",
    "from src.utils.flow_calculations import *\n",
    "from src.utils.restoration import *\n",
    "from src.utils.truncated_packet import *\n",
    "from src.operations.size_perturbation_logic import *\n",
    "from src.operations.timing_perturbation_logic import *\n",
    "from src.operations.calculate_fitness import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_best_on_packets(packets, max_fitness, flow_id):\n",
    "\n",
    "    index = max_fitness[1][2]\n",
    "    #print(index)\n",
    "    if index == 0:\n",
    "        return packets\n",
    "    \n",
    "    params = max_fitness[1][1][index-1] # best technique\n",
    "    #print(f'saved:{params}')\n",
    "    \n",
    "    if index == 1:\n",
    "        #print('packet_size_nor')\n",
    "        #print(f'size: {prepare_size_stats(packets, flow_id)}')\n",
    "        \n",
    "        return adjust_packet_size(packets, flow_id, params['direction'], 'normal', params['scaling_factor'], params['focus_point'])\n",
    "        \n",
    "                \n",
    "    elif index == 2:\n",
    "        #print('packet_size_uni')\n",
    "        #print(f'size: {prepare_size_stats(packets, flow_id)}')\n",
    "        return adjust_packet_size(packets, flow_id, params['direction'], 'uniform', params['scaling_factor'], params['focus_point'])\n",
    "                \n",
    "    elif index == 3:\n",
    "        #print('packet_time_norm')\n",
    "        #print(f'time: {prepare_timing_stats(packets, flow_id)}')\n",
    "        return apply_time_perturbation_with_focus(packets, flow_id, 'normal', params['focus_point'], params['scaling_factor'])\n",
    "                \n",
    "    elif index == 4:\n",
    "        #print('packet_time_uni')\n",
    "        #print(f'time: {prepare_timing_stats(packets, flow_id)}')\n",
    "        return apply_time_perturbation_with_focus(packets, flow_id, 'uniform', params['focus_point'], params['scaling_factor'])\n",
    "        \n",
    "    return packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygad\n",
    "import copy\n",
    "\n",
    "class FlowOptimizer:\n",
    "    def __init__(self, packets, model_name, flow_id, max_fitness):\n",
    "        self.packets = packets\n",
    "        self.model_name = model_name\n",
    "        self.flow_id = flow_id\n",
    "        self.ga_instance = None\n",
    "        self.max_fitness = max_fitness\n",
    "        \n",
    "    def apply_best_solution(self):\n",
    "        best_solution, best_solution_fitness, _ = self.ga_instance.best_solution()\n",
    "        best_operations = self.decode_operations(best_solution)\n",
    "        # Stosujemy najlepsze operacje do pakietów\n",
    "        self.best_modified_packets = self.apply_operations_to_packets(best_operations, self.packets, self.flow_id)\n",
    "        # Możesz zwrócić więcej informacji o najlepszym rozwiązaniu, jeśli potrzebujesz\n",
    "        return self.best_modified_packets, best_solution_fitness\n",
    "\n",
    "    def on_generation(self, ga_instance):\n",
    "        current_population = ga_instance.population\n",
    "        print(f\"Flow ID: {self.flow_id}, Pokolenie: {ga_instance.generations_completed}\")\n",
    "        #print(\"Aktualna populacja:\")\n",
    "        #for solution in current_population:\n",
    "        #    print(solution)\n",
    "        # Tworzymy wiadomość zawierającą interesujące nas informacje\n",
    "        message = f\"Najlepszy wynik: {ga_instance.best_solution()[1]}\\n\"\n",
    "        print(message)  # Wyświetlamy informacje na konsoli\n",
    "        \n",
    "        # Zapisujemy te same informacje do pliku\n",
    "        #with open(f\"progress_for_flow_{self.flow_id}.txt\", \"a\") as file:\n",
    "        #    file.write(message)\n",
    "            \n",
    "    def on_fitness(self, ga_instance):\n",
    "        ga_instance.plot_fitness()\n",
    "\n",
    "    def fitness_function(self, ga_instance, solution, solution_idx):\n",
    "        \n",
    "        fitnesses = []\n",
    "        target_fitnesses = []\n",
    "        \n",
    "        operations = self.decode_operations(solution)\n",
    "        \n",
    "        modified_packets = self.packets # always check nop\n",
    "        sizing_stats = prepare_size_stats(modified_packets, self.flow_id)\n",
    "        timing_stats = prepare_timing_stats(modified_packets, self.flow_id)\n",
    "        fitnesses.append(1.0 - predict_single_flow(self.model_name, sizing_stats, timing_stats))\n",
    "        target_fitnesses.append(1.0 - predict_single_flow_target(self.model_name, sizing_stats, timing_stats))\n",
    "        \n",
    "        modified_packets = self.apply_operations_to_packets(operations[0], op_type = 'size_norm')\n",
    "        sizing_stats = prepare_size_stats(modified_packets, self.flow_id)\n",
    "        #print(sizing_stats)\n",
    "        timing_stats = prepare_timing_stats(modified_packets, self.flow_id)\n",
    "        fitnesses.append(1.0 - predict_single_flow(self.model_name, sizing_stats, timing_stats))\n",
    "        target_fitnesses.append(1.0 - predict_single_flow_target(self.model_name, sizing_stats, timing_stats))\n",
    "        \n",
    "        modified_packets = self.apply_operations_to_packets(operations[1], op_type = 'size_uni')\n",
    "        sizing_stats = prepare_size_stats(modified_packets, self.flow_id)\n",
    "        timing_stats = prepare_timing_stats(modified_packets, self.flow_id)\n",
    "        fitnesses.append(1.0 - predict_single_flow(self.model_name, sizing_stats, timing_stats))\n",
    "        target_fitnesses.append(1.0 - predict_single_flow_target(self.model_name, sizing_stats, timing_stats))\n",
    "        \n",
    "        modified_packets = self.apply_operations_to_packets(operations[2], op_type = 'time_norm')\n",
    "        sizing_stats = prepare_size_stats(modified_packets, self.flow_id)\n",
    "        timing_stats = prepare_timing_stats(modified_packets, self.flow_id)\n",
    "        fitnesses.append(1.0 - predict_single_flow(self.model_name, sizing_stats, timing_stats))\n",
    "        target_fitnesses.append(1.0 - predict_single_flow_target(self.model_name, sizing_stats, timing_stats))\n",
    "        \n",
    "        modified_packets = self.apply_operations_to_packets(operations[3], op_type = 'time_uni')\n",
    "        sizing_stats = prepare_size_stats(modified_packets, self.flow_id)\n",
    "        timing_stats = prepare_timing_stats(modified_packets, self.flow_id)\n",
    "        fitnesses.append(1.0 - predict_single_flow(self.model_name, sizing_stats, timing_stats))\n",
    "        target_fitnesses.append(1.0 - predict_single_flow_target(self.model_name, sizing_stats, timing_stats))\n",
    "        \n",
    "        #print(fitnesses)\n",
    "        max_fitness_value = max(fitnesses)\n",
    "        max_fitness_index = fitnesses.index(max_fitness_value)\n",
    "        #print(max_fitness_index)\n",
    "\n",
    "        max_fitness_target = target_fitnesses[max_fitness_index]\n",
    "\n",
    "        \n",
    "        # Zapisz maksymalną wartość fitness i indeks, jeśli jest większa niż obecnie zapisana maksymalna wartość\n",
    "        print(self.max_fitness)\n",
    "        if max_fitness_value > self.max_fitness[0][0]:\n",
    "            #print(f'CONDITION max: {max_fitness_value}, prev: {self.max_fitness[0]}')\n",
    "            self.max_fitness = [[max_fitness_value, max_fitness_target], self.decode_operations(solution), max_fitness_index]\n",
    "            #print(f'self:{self.max_fitness[0]}')\n",
    "        \n",
    "        #print(max_fitness_value)\n",
    "        #print(f'solution:{solution}')\n",
    "        #print(f'finresses: {fitnesses}, solution:{self.decode_operations(solution)}')\n",
    "        \n",
    "        return max_fitness_value\n",
    "\n",
    "    def optimize_for_flow(self):\n",
    "        \n",
    "        #with open(f\"progress_for_flow_{self.flow_id}.txt\", \"w\") as file:\n",
    "        #    file.write(\"\")  # Tylko czyszczenie zawartości pliku\n",
    "            \n",
    "        ga_instance = pygad.GA(num_generations=3,\n",
    "                               num_parents_mating=1,\n",
    "                               fitness_func=self.fitness_function,\n",
    "                               sol_per_pop=4,                          \n",
    "                               mutation_probability=0.6,\n",
    "     #                          initial_population=np.array([\n",
    "    #        [4.14645349, 9.76164109, 1.67182642, 2.09448623, 1, 0],\n",
    "   #         [2.80090477, 4.24612439, 1.41426283, 1.12520895, 2, 1],\n",
    "  #          [1.31962389, 1.47082939, 2.15440462, 1.3226799, 0, 2]\n",
    " #       ]),\n",
    "                               on_generation=self.on_generation,\n",
    "                               num_genes=6,  # Przykład, dostosuj do ilości operacji\n",
    "                               gene_space = [{'low': 1.1, 'high': 5.0},  # size normal\n",
    "                                             {'low': 0.25, 'high': 5.0}, # packet size uniform\n",
    "                                             {'low': 1.1, 'high': 3.0}, # time normal\n",
    "                                             {'low': 0.4, 'high': 2.4}, # time uniform\n",
    "                                            [0,1,2],  # direction\n",
    "                                            [0,1,2]]) # focus point \n",
    "\n",
    "        \n",
    "        ga_instance.run()\n",
    "        return ga_instance, self.max_fitness\n",
    "\n",
    "    def decode_operations(self, genotype):\n",
    "        operations = []\n",
    "        \n",
    "\n",
    "        focus_point_map = {0: \"start\", 1: \"middle\", 2: \"end\"}\n",
    "        focus_point = None\n",
    "        focus_point_index = genotype[5] \n",
    "        focus_point = focus_point_map.get(focus_point_index, None)  \n",
    "\n",
    "        size_norm = {\n",
    "            'scaling_factor': genotype[0],\n",
    "            'direction': genotype[4],\n",
    "            'focus_point': focus_point\n",
    "        }\n",
    "        \n",
    "        size_uni = {\n",
    "            'scaling_factor': genotype[1],\n",
    "            'direction': genotype[4],\n",
    "            'focus_point': focus_point\n",
    "        }\n",
    "        \n",
    "        time_norm = {\n",
    "            'scaling_factor': genotype[2],\n",
    "            'direction': genotype[4],\n",
    "            'focus_point': focus_point\n",
    "        }\n",
    "        \n",
    "        \n",
    "        time_uni = {\n",
    "            'scaling_factor': genotype[3],\n",
    "            'direction': genotype[4],\n",
    "            'focus_point': focus_point\n",
    "        }  \n",
    "            \n",
    "            \n",
    "        \n",
    "        operations = [size_norm, size_uni, time_norm, time_uni]\n",
    "        return operations \n",
    "        \n",
    "\n",
    "\n",
    "    def apply_operations_to_packets(self, operation, op_type):\n",
    "\n",
    "        #packets = copy.deepcopy(self.packets) # deep copy for testing\n",
    "        #print(f'size: {prepare_size_stats(self.packets, self.flow_id)}')\n",
    "        #print(f'time: {prepare_timing_stats(self.packets, self.flow_id)}')\n",
    "\n",
    "\n",
    "        scaling_factor = operation['scaling_factor']\n",
    "        direction = operation.get('direction')\n",
    "        focus_point = operation.get('focus_point')\n",
    "        \n",
    "\n",
    "        if op_type == 'size_norm':\n",
    "            packets = adjust_packet_size_deepcopy(self.packets, self.flow_id, direction, 'normal', scaling_factor, focus_point)\n",
    "            \n",
    "            \n",
    "        elif op_type == 'size_uni':\n",
    "            packets = adjust_packet_size_deepcopy(self.packets, self.flow_id, direction, 'uniform', scaling_factor, focus_point)\n",
    "            \n",
    "        elif op_type == 'time_norm':\n",
    "            packets = apply_time_perturbation_with_focus_deepcopy(self.packets, self.flow_id, 'normal', focus_point, scaling_factor)\n",
    "            \n",
    "        elif op_type == 'time_uni':\n",
    "            #print(f'przed: {prepare_timing_stats(self.packets, self.flow_id)}')\n",
    "            packets = apply_time_perturbation_with_focus_deepcopy(self.packets, self.flow_id, 'uniform', focus_point, scaling_factor)\n",
    "            #print(f'po: {prepare_timing_stats(packets, self.flow_id)}')\n",
    "            \n",
    "        #print(packets)\n",
    "        return packets\n",
    "    \n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "    #print(optimizer.best_solution())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_object_pickle(obj, filepath):\n",
    "    \"\"\"\n",
    "    Saves an object to a specified filepath using pickle.\n",
    "    \n",
    "    Parameters:\n",
    "    - obj: The object to save.\n",
    "    - filepath: The path to the file where the object should be saved.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    \n",
    "    with open(filepath, 'wb') as pickle_file:  # Note 'wb' for writing in binary mode\n",
    "        pickle.dump(obj, pickle_file)\n",
    "\n",
    "def load_object_pickle(filepath):\n",
    "    \"\"\"\n",
    "    Loads an object from a specified filepath using pickle.\n",
    "    \n",
    "    Parameters:\n",
    "    - filepath: The path to the file from which to load the object.\n",
    "    \n",
    "    Returns:\n",
    "    - The loaded object.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'rb') as pickle_file:  # Note 'rb' for reading in binary mode\n",
    "        obj = pickle.load(pickle_file)\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "\n",
    "captures = ['botnet-capture-20110810-neris', 'botnet-capture-20110811-neris', 'botnet-capture-20110812-rbot', 'botnet-capture-20110815-fast-flux-2', 'botnet-capture-20110815-fast-flux',\n",
    "            'botnet-capture-20110815-rbot-dos', 'botnet-capture-20110816-donbot', 'botnet-capture-20110816-qvod', 'botnet-capture-20110816-sogou',\n",
    "            'botnet-capture-20110817-bot', 'botnet-capture-20110818-bot-2', 'botnet-capture-20110819-bot']\n",
    "\n",
    "for capture in captures:\n",
    "    capture = \"botnet-capture-20110816-donbot\"\n",
    "\n",
    "    modified_pcap_path = f\"../data/interim/{capture}_perturbed.pcapng\"\n",
    "    pcap_file_path = f'../data/raw/{capture}.pcap'\n",
    "    save_object_path = f\"../data/metadata/ga_rf_fitness_{capture}_perturbed.pkl\"\n",
    "    model_name = f'{capture}'\n",
    "\n",
    "    truncated_packets = create_truncated_packets_from_pcap(pcap_file_path)\n",
    "    truncated_packets = assign_flow_ids_to_packets(truncated_packets)\n",
    "    truncated_packets = undersample_flows_with_distribution(truncated_packets, 100)\n",
    "    optimizers = []  # Lista na instancje optymalizatorów\n",
    "\n",
    "    flow_ids = set(packet.flow_id for packet in truncated_packets)\n",
    "\n",
    "    extern = []\n",
    "    for flow_id in flow_ids:\n",
    "        inter = []\n",
    "        extern.append(inter)\n",
    "        max_fitness = [0, [[0,0],0,0], 0]\n",
    "        previous_max = 0\n",
    "        for i in range(0, 5):\n",
    "            #timing_stats = prepare_timing_stats(truncated_packets, flow_id)\n",
    "            #sizing_stats = prepare_size_stats(truncated_packets,flow_id)\n",
    "            #print(predict_single_flow(model_name, sizing_stats, timing_stats))\n",
    "            truncated_packets_copy = copy.deepcopy(truncated_packets)\n",
    "            optimizer = FlowOptimizer(truncated_packets_copy, model_name, flow_id, max_fitness[1]) \n",
    "\n",
    "            max_fitness = optimizer.optimize_for_flow()\n",
    "            inter.append(max_fitness)\n",
    "            \n",
    "            #print(f'nowy: {max_fitness[1][0]} stary: {previous_max}')\n",
    "            #print(max_fitness[1][0][0])\n",
    "            if max_fitness[1][0][0] > previous_max: # if highest value higher than before\n",
    "                \n",
    "                truncated_packets = apply_best_on_packets(truncated_packets, max_fitness, flow_id)\n",
    "                #print(f'size after: {prepare_size_stats(truncated_packets, flow_id)}')\n",
    "                #print(f'time after: {prepare_timing_stats(truncated_packets, flow_id)}')\n",
    "                \n",
    "                previous_max = max_fitness[1][0][0]\n",
    "            \n",
    "\n",
    "    modify_and_write_packets_one_by_one(pcap_file_path, modified_pcap_path, truncated_packets)\n",
    "    \n",
    "    save_object_pickle(extern, save_object_path) # at the end of each capture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
