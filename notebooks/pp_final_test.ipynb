{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: No IPv4 address found on anpi1 !\n",
      "WARNING: No IPv4 address found on anpi0 !\n",
      "WARNING: more No IPv4 address found on en3 !\n"
     ]
    }
   ],
   "source": [
    "# saved files testing\n",
    "from src.utils.flow import *\n",
    "from src.utils.flow_calculations import *\n",
    "from src.utils.restoration import *\n",
    "from src.utils.truncated_packet import *\n",
    "from src.operations.size_perturbation_logic import *\n",
    "from src.operations.timing_perturbation_logic import *\n",
    "from src.operations.calculate_fitness import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_best_on_packets(packets, max_fitness, flow_id):\n",
    "\n",
    "    index = max_fitness[1][2]\n",
    "    #print(index)\n",
    "    if index == 0:\n",
    "        return packets\n",
    "    \n",
    "    params = max_fitness[1][1][index-1] # best technique\n",
    "    #print(f'saved:{params}')\n",
    "    \n",
    "    if index == 1:\n",
    "        #print('packet_size_nor')\n",
    "        #print(f'size: {prepare_size_stats(packets, flow_id)}')\n",
    "        \n",
    "        return adjust_packet_size(packets, flow_id, params['direction'], 'normal', params['scaling_factor'], params['focus_point'])\n",
    "        \n",
    "                \n",
    "    elif index == 2:\n",
    "        #print('packet_size_uni')\n",
    "        #print(f'size: {prepare_size_stats(packets, flow_id)}')\n",
    "        return adjust_packet_size(packets, flow_id, params['direction'], 'uniform', params['scaling_factor'], params['focus_point'])\n",
    "                \n",
    "    elif index == 3:\n",
    "        #print('packet_time_norm')\n",
    "        #print(f'time: {prepare_timing_stats(packets, flow_id)}')\n",
    "        return apply_time_perturbation_with_focus(packets, flow_id, 'normal', params['focus_point'], params['scaling_factor'])\n",
    "                \n",
    "    elif index == 4:\n",
    "        #print('packet_time_uni')\n",
    "        #print(f'time: {prepare_timing_stats(packets, flow_id)}')\n",
    "        return apply_time_perturbation_with_focus(packets, flow_id, 'uniform', params['focus_point'], params['scaling_factor'])\n",
    "        \n",
    "    return packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygad\n",
    "import copy\n",
    "\n",
    "class FlowOptimizer:\n",
    "    def __init__(self, packets, model_name, flow_id, max_fitness):\n",
    "        self.packets = packets\n",
    "        self.model_name = model_name\n",
    "        self.flow_id = flow_id\n",
    "        self.ga_instance = None\n",
    "        self.max_fitness = max_fitness\n",
    "        \n",
    "    def apply_best_solution(self):\n",
    "        best_solution, best_solution_fitness, _ = self.ga_instance.best_solution()\n",
    "        best_operations = self.decode_operations(best_solution)\n",
    "        # Stosujemy najlepsze operacje do pakietów\n",
    "        self.best_modified_packets = self.apply_operations_to_packets(best_operations, self.packets, self.flow_id)\n",
    "        # Możesz zwrócić więcej informacji o najlepszym rozwiązaniu, jeśli potrzebujesz\n",
    "        return self.best_modified_packets, best_solution_fitness\n",
    "\n",
    "    def on_generation(self, ga_instance):\n",
    "        #current_population = ga_instance.population\n",
    "        #print(f\"Flow ID: {self.flow_id}, Pokolenie: {ga_instance.generations_completed}\")\n",
    "        #print(\"Aktualna populacja:\")\n",
    "        #for solution in current_population:\n",
    "        #    print(solution)\n",
    "        # Tworzymy wiadomość zawierającą interesujące nas informacje\n",
    "        #message = f\"Najlepszy wynik: {ga_instance.best_solution()[1]}\\n\"\n",
    "        #print(message)  # Wyświetlamy informacje na konsoli\n",
    "        \n",
    "        # Zapisujemy te same informacje do pliku\n",
    "        #with open(f\"progress_for_flow_{self.flow_id}.txt\", \"a\") as file:\n",
    "        #    file.write(message)\n",
    "        pass\n",
    "            \n",
    "    def on_fitness(self, ga_instance):\n",
    "        ga_instance.plot_fitness()\n",
    "\n",
    "    def fitness_function(self, ga_instance, solution, solution_idx):\n",
    "        \n",
    "        fitnesses = []\n",
    "        target_fitnesses = []\n",
    "        \n",
    "        operations = self.decode_operations(solution)\n",
    "        \n",
    "        modified_packets = self.packets # always check nop\n",
    "        sizing_stats = prepare_size_stats(modified_packets, self.flow_id)\n",
    "        timing_stats = prepare_timing_stats(modified_packets, self.flow_id)\n",
    "        fitnesses.append(1.0 - predict_single_flow(self.model_name, sizing_stats, timing_stats))\n",
    "        target_fitnesses.append(1.0 - predict_single_flow_target(self.model_name, sizing_stats, timing_stats))\n",
    "        \n",
    "        modified_packets = self.apply_operations_to_packets(operations[0], op_type = 'size_norm')\n",
    "        sizing_stats = prepare_size_stats(modified_packets, self.flow_id)\n",
    "        #print(sizing_stats)\n",
    "        timing_stats = prepare_timing_stats(modified_packets, self.flow_id)\n",
    "        fitnesses.append(1.0 - predict_single_flow(self.model_name, sizing_stats, timing_stats))\n",
    "        target_fitnesses.append(1.0 - predict_single_flow_target(self.model_name, sizing_stats, timing_stats))\n",
    "        \n",
    "        modified_packets = self.apply_operations_to_packets(operations[1], op_type = 'size_uni')\n",
    "        sizing_stats = prepare_size_stats(modified_packets, self.flow_id)\n",
    "        timing_stats = prepare_timing_stats(modified_packets, self.flow_id)\n",
    "        fitnesses.append(1.0 - predict_single_flow(self.model_name, sizing_stats, timing_stats))\n",
    "        target_fitnesses.append(1.0 - predict_single_flow_target(self.model_name, sizing_stats, timing_stats))\n",
    "        \n",
    "        modified_packets = self.apply_operations_to_packets(operations[2], op_type = 'time_norm')\n",
    "        sizing_stats = prepare_size_stats(modified_packets, self.flow_id)\n",
    "        timing_stats = prepare_timing_stats(modified_packets, self.flow_id)\n",
    "        fitnesses.append(1.0 - predict_single_flow(self.model_name, sizing_stats, timing_stats))\n",
    "        target_fitnesses.append(1.0 - predict_single_flow_target(self.model_name, sizing_stats, timing_stats))\n",
    "        \n",
    "        modified_packets = self.apply_operations_to_packets(operations[3], op_type = 'time_uni')\n",
    "        sizing_stats = prepare_size_stats(modified_packets, self.flow_id)\n",
    "        timing_stats = prepare_timing_stats(modified_packets, self.flow_id)\n",
    "        fitnesses.append(1.0 - predict_single_flow(self.model_name, sizing_stats, timing_stats))\n",
    "        target_fitnesses.append(1.0 - predict_single_flow_target(self.model_name, sizing_stats, timing_stats))\n",
    "        \n",
    "        #print(fitnesses)\n",
    "        max_fitness_value = max(fitnesses)\n",
    "        max_fitness_index = fitnesses.index(max_fitness_value)\n",
    "        #print(max_fitness_index)\n",
    "\n",
    "        max_fitness_target = target_fitnesses[max_fitness_index]\n",
    "\n",
    "        \n",
    "        # Zapisz maksymalną wartość fitness i indeks, jeśli jest większa niż obecnie zapisana maksymalna wartość\n",
    "        #print(self.max_fitness)\n",
    "        if max_fitness_value > self.max_fitness[0][0]:\n",
    "            #print(f'CONDITION max: {max_fitness_value}, prev: {self.max_fitness[0]}')\n",
    "            self.max_fitness = [[max_fitness_value, max_fitness_target], self.decode_operations(solution), max_fitness_index]\n",
    "            #print(f'self:{self.max_fitness[0]}')\n",
    "        \n",
    "        #print(max_fitness_value)\n",
    "        #print(f'solution:{solution}')\n",
    "        #print(f'finresses: {fitnesses}, solution:{self.decode_operations(solution)}')\n",
    "        \n",
    "        return max_fitness_value\n",
    "\n",
    "    def optimize_for_flow(self):\n",
    "        \n",
    "        #with open(f\"progress_for_flow_{self.flow_id}.txt\", \"w\") as file:\n",
    "        #    file.write(\"\")  # Tylko czyszczenie zawartości pliku\n",
    "            \n",
    "        ga_instance = pygad.GA(num_generations=2,\n",
    "                               num_parents_mating=1,\n",
    "                               fitness_func=self.fitness_function,\n",
    "                               sol_per_pop=3,                          \n",
    "                               mutation_probability=0.7,\n",
    "                               suppress_warnings= True,\n",
    "     #                          initial_population=np.array([\n",
    "    #        [4.14645349, 9.76164109, 1.67182642, 2.09448623, 1, 0],\n",
    "   #         [2.80090477, 4.24612439, 1.41426283, 1.12520895, 2, 1],\n",
    "  #          [1.31962389, 1.47082939, 2.15440462, 1.3226799, 0, 2]\n",
    " #       ]),\n",
    "                               on_generation=self.on_generation,\n",
    "                               num_genes=6,  # Przykład, dostosuj do ilości operacji\n",
    "                               gene_space = [{'low': 1.1, 'high': 5.0},  # size normal\n",
    "                                             {'low': 0.25, 'high': 5.0}, # packet size uniform\n",
    "                                             {'low': 1.1, 'high': 3.0}, # time normal\n",
    "                                             {'low': 0.4, 'high': 2.4}, # time uniform\n",
    "                                            [0,1,2],  # direction\n",
    "                                            [0,1,2]]) # focus point \n",
    "\n",
    "        \n",
    "        ga_instance.run()\n",
    "        return ga_instance, self.max_fitness\n",
    "\n",
    "    def decode_operations(self, genotype):\n",
    "        operations = []\n",
    "        \n",
    "\n",
    "        focus_point_map = {0: \"start\", 1: \"middle\", 2: \"end\"}\n",
    "        focus_point = None\n",
    "        focus_point_index = genotype[5] \n",
    "        focus_point = focus_point_map.get(focus_point_index, None)  \n",
    "\n",
    "        size_norm = {\n",
    "            'scaling_factor': genotype[0],\n",
    "            'direction': genotype[4],\n",
    "            'focus_point': focus_point\n",
    "        }\n",
    "        \n",
    "        size_uni = {\n",
    "            'scaling_factor': genotype[1],\n",
    "            'direction': genotype[4],\n",
    "            'focus_point': focus_point\n",
    "        }\n",
    "        \n",
    "        time_norm = {\n",
    "            'scaling_factor': genotype[2],\n",
    "            'direction': genotype[4],\n",
    "            'focus_point': focus_point\n",
    "        }\n",
    "        \n",
    "        \n",
    "        time_uni = {\n",
    "            'scaling_factor': genotype[3],\n",
    "            'direction': genotype[4],\n",
    "            'focus_point': focus_point\n",
    "        }  \n",
    "            \n",
    "            \n",
    "        \n",
    "        operations = [size_norm, size_uni, time_norm, time_uni]\n",
    "        return operations \n",
    "        \n",
    "\n",
    "\n",
    "    def apply_operations_to_packets(self, operation, op_type):\n",
    "\n",
    "        #packets = copy.deepcopy(self.packets) # deep copy for testing\n",
    "        #print(f'size: {prepare_size_stats(self.packets, self.flow_id)}')\n",
    "        #print(f'time: {prepare_timing_stats(self.packets, self.flow_id)}')\n",
    "\n",
    "\n",
    "        scaling_factor = operation['scaling_factor']\n",
    "        direction = operation.get('direction')\n",
    "        focus_point = operation.get('focus_point')\n",
    "        \n",
    "\n",
    "        if op_type == 'size_norm':\n",
    "            packets = adjust_packet_size_deepcopy(self.packets, self.flow_id, direction, 'normal', scaling_factor, focus_point)\n",
    "            \n",
    "            \n",
    "        elif op_type == 'size_uni':\n",
    "            packets = adjust_packet_size_deepcopy(self.packets, self.flow_id, direction, 'uniform', scaling_factor, focus_point)\n",
    "            \n",
    "        elif op_type == 'time_norm':\n",
    "            packets = apply_time_perturbation_with_focus_deepcopy(self.packets, self.flow_id, 'normal', focus_point, scaling_factor)\n",
    "            \n",
    "        elif op_type == 'time_uni':\n",
    "            #print(f'przed: {prepare_timing_stats(self.packets, self.flow_id)}')\n",
    "            packets = apply_time_perturbation_with_focus_deepcopy(self.packets, self.flow_id, 'uniform', focus_point, scaling_factor)\n",
    "            #print(f'po: {prepare_timing_stats(packets, self.flow_id)}')\n",
    "            \n",
    "        #print(packets)\n",
    "        return packets\n",
    "    \n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "    #print(optimizer.best_solution())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_object_pickle(obj, filepath):\n",
    "    \"\"\"\n",
    "    Saves an object to a specified filepath using pickle.\n",
    "    \n",
    "    Parameters:\n",
    "    - obj: The object to save.\n",
    "    - filepath: The path to the file where the object should be saved.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    \n",
    "    with open(filepath, 'wb') as pickle_file:  # Note 'wb' for writing in binary mode\n",
    "        pickle.dump(obj, pickle_file)\n",
    "\n",
    "def load_object_pickle(filepath):\n",
    "    \"\"\"\n",
    "    Loads an object from a specified filepath using pickle.\n",
    "    \n",
    "    Parameters:\n",
    "    - filepath: The path to the file from which to load the object.\n",
    "    \n",
    "    Returns:\n",
    "    - The loaded object.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'rb') as pickle_file:  # Note 'rb' for reading in binary mode\n",
    "        obj = pickle.load(pickle_file)\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on: botnet-capture-20110816-donbot\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/raw/botnet-capture-20110816-donbot.pcap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/cyberaml/lib/python3.11/site-packages/scapy/utils.py:1209\u001b[0m, in \u001b[0;36mPcapReader_metaclass.open\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     fdesc \u001b[38;5;241m=\u001b[39m \u001b[43mgzip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: _ByteStream\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m     magic \u001b[38;5;241m=\u001b[39m fdesc\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cyberaml/lib/python3.11/gzip.py:58\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike)):\n\u001b[0;32m---> 58\u001b[0m     binary_file \u001b[38;5;241m=\u001b[39m \u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgz_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompresslevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/cyberaml/lib/python3.11/gzip.py:174\u001b[0m, in \u001b[0;36mGzipFile.__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmyfileobj \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, mode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/raw/botnet-capture-20110816-donbot.pcap'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m save_object_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/metadata/ga_rf_fitness_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcapture\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_perturbed.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcapture\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 17\u001b[0m truncated_packets \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_truncated_packets_from_pcap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcap_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m truncated_packets \u001b[38;5;241m=\u001b[39m assign_flow_ids_to_packets(truncated_packets)\n\u001b[1;32m     19\u001b[0m truncated_packets \u001b[38;5;241m=\u001b[39m undersample_flows_with_distribution(truncated_packets, \u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/github/CyberAML/src/utils/truncated_packet.py:52\u001b[0m, in \u001b[0;36mcreate_truncated_packets_from_pcap\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_truncated_packets_from_pcap\u001b[39m(file_path):\n\u001b[1;32m     51\u001b[0m     truncated_packets \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 52\u001b[0m     cap \u001b[38;5;241m=\u001b[39m \u001b[43mrdpcap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m packet_number, scapy_packet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cap, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m IP \u001b[38;5;129;01min\u001b[39;00m scapy_packet \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m     56\u001b[0m             scapy_packet\u001b[38;5;241m.\u001b[39mhaslayer(TCP) \u001b[38;5;129;01mor\u001b[39;00m scapy_packet\u001b[38;5;241m.\u001b[39mhaslayer(UDP)\n\u001b[1;32m     57\u001b[0m         ):\n",
      "File \u001b[0;32m~/miniconda3/envs/cyberaml/lib/python3.11/site-packages/scapy/utils.py:1144\u001b[0m, in \u001b[0;36mrdpcap\u001b[0;34m(filename, count)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Read a pcap or pcapng file and return a packet list\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03m:param count: read only <count> packets\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# Rant: Our complicated use of metaclasses and especially the\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m# __call__ function is, of course, not supported by MyPy.\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;66;03m# One day we should simplify this mess and use a much simpler\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;66;03m# layout that will actually be supported and properly dissected.\u001b[39;00m\n\u001b[0;32m-> 1144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPcapReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fdesc:  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fdesc\u001b[38;5;241m.\u001b[39mread_all(count\u001b[38;5;241m=\u001b[39mcount)\n",
      "File \u001b[0;32m~/miniconda3/envs/cyberaml/lib/python3.11/site-packages/scapy/utils.py:1179\u001b[0m, in \u001b[0;36mPcapReader_metaclass.__call__\u001b[0;34m(cls, filename)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a cls instance, use the `alternative` if that\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;124;03mfails.\u001b[39;00m\n\u001b[1;32m   1176\u001b[0m \n\u001b[1;32m   1177\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1178\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__bases__\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m)\n\u001b[0;32m-> 1179\u001b[0m filename, fdesc, magic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m magic:\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Scapy_Exception(\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data could be read!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1183\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/cyberaml/lib/python3.11/site-packages/scapy/utils.py:1212\u001b[0m, in \u001b[0;36mPcapReader_metaclass.open\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1210\u001b[0m         magic \u001b[38;5;241m=\u001b[39m fdesc\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m:\n\u001b[0;32m-> 1212\u001b[0m         fdesc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1213\u001b[0m         magic \u001b[38;5;241m=\u001b[39m fdesc\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/raw/botnet-capture-20110816-donbot.pcap'"
     ]
    }
   ],
   "source": [
    "# main\n",
    "\n",
    "captures = ['botnet-capture-20110816-donbot', 'botnet-capture-20110815-rbot-dos']\n",
    "# 'botnet-capture-20110810-neris', 'botnet-capture-20110811-neris',\n",
    "# 'botnet-capture-20110816-qvod', 'botnet-capture-20110816-sogou',\n",
    "#            'botnet-capture-20110817-bot', 'botnet-capture-20110818-bot-2', 'botnet-capture-20110819-bot'\n",
    "# 'botnet-capture-20110812-rbot', 'botnet-capture-20110815-fast-flux-2', 'botnet-capture-20110815-fast-flux',\n",
    "#            'botnet-capture-20110815-rbot-dos',\n",
    "for capture in captures:\n",
    "    print(f'working on: {capture}')\n",
    "\n",
    "    modified_pcap_path = f\"../data/interim/{capture}_perturbed.pcapng\"\n",
    "    pcap_file_path = f'../data/raw/{capture}.pcap'\n",
    "    save_object_path = f\"../data/metadata/ga_rf_fitness_{capture}_perturbed.pkl\"\n",
    "    model_name = f'{capture}'\n",
    "\n",
    "    truncated_packets = create_truncated_packets_from_pcap(pcap_file_path)\n",
    "    truncated_packets = assign_flow_ids_to_packets(truncated_packets)\n",
    "    truncated_packets = undersample_flows_with_distribution(truncated_packets, 100)\n",
    "    optimizers = []  # Lista na instancje optymalizatorów\n",
    "\n",
    "    flow_ids = set(packet.flow_id for packet in truncated_packets)\n",
    "\n",
    "    extern = []\n",
    "    for flow_id in flow_ids:\n",
    "        inter = []\n",
    "        extern.append(inter)\n",
    "        max_fitness = [0, [[0,0],0,0], 0]\n",
    "        previous_max = 0\n",
    "        for i in range(0, 5):\n",
    "            #timing_stats = prepare_timing_stats(truncated_packets, flow_id)\n",
    "            #sizing_stats = prepare_size_stats(truncated_packets,flow_id)\n",
    "            #print(predict_single_flow(model_name, sizing_stats, timing_stats))\n",
    "            truncated_packets_copy = copy.deepcopy(truncated_packets)\n",
    "            optimizer = FlowOptimizer(truncated_packets_copy, model_name, flow_id, max_fitness[1]) \n",
    "\n",
    "            max_fitness = optimizer.optimize_for_flow()\n",
    "            inter.append(max_fitness)\n",
    "            \n",
    "            #print(f'nowy: {max_fitness[1][0]} stary: {previous_max}')\n",
    "            #print(max_fitness[1][0][0])\n",
    "            if max_fitness[1][0][0] > previous_max: # if highest value higher than before\n",
    "                \n",
    "                truncated_packets = apply_best_on_packets(truncated_packets, max_fitness, flow_id)\n",
    "                #print(f'size after: {prepare_size_stats(truncated_packets, flow_id)}')\n",
    "                #print(f'time after: {prepare_timing_stats(truncated_packets, flow_id)}')\n",
    "                \n",
    "                previous_max = max_fitness[1][0][0]\n",
    "            \n",
    "\n",
    "    modify_and_write_packets_one_by_one(pcap_file_path, modified_pcap_path, truncated_packets)\n",
    "    \n",
    "    save_object_pickle(extern, save_object_path) # at the end of each capture \n",
    "    print(f\"saved: {capture}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object_pickle(extern, save_object_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(<pygad.pygad.GA at 0x45c12f890>,\n",
       "   [[0.30386197124173275, 0.014641077630512256],\n",
       "    [{'scaling_factor': 3.033535322253271,\n",
       "      'direction': 0.0,\n",
       "      'focus_point': 'start'},\n",
       "     {'scaling_factor': 1.0375247582506013,\n",
       "      'direction': 0.0,\n",
       "      'focus_point': 'start'},\n",
       "     {'scaling_factor': 1.1962537248024392,\n",
       "      'direction': 0.0,\n",
       "      'focus_point': 'start'},\n",
       "     {'scaling_factor': 1.625662429595672,\n",
       "      'direction': 0.0,\n",
       "      'focus_point': 'start'}],\n",
       "    1]),\n",
       "  (<pygad.pygad.GA at 0x7a830bcd0>,\n",
       "   [[0.3751197547345124, 0.08124542323647699],\n",
       "    [{'scaling_factor': 2.4503433288864778,\n",
       "      'direction': 0.0,\n",
       "      'focus_point': 'end'},\n",
       "     {'scaling_factor': 1.8667769478674892,\n",
       "      'direction': 0.0,\n",
       "      'focus_point': 'end'},\n",
       "     {'scaling_factor': 1.1262446535541992,\n",
       "      'direction': 0.0,\n",
       "      'focus_point': 'end'},\n",
       "     {'scaling_factor': 1.1545402499224586,\n",
       "      'direction': 0.0,\n",
       "      'focus_point': 'end'}],\n",
       "    2]),\n",
       "  (<pygad.pygad.GA at 0x2839afd50>,\n",
       "   [[0.3986982356346338, 0.10054591944759494],\n",
       "    [{'scaling_factor': 3.0523909145775896,\n",
       "      'direction': 1.0,\n",
       "      'focus_point': 'end'},\n",
       "     {'scaling_factor': 2.5298385186996732,\n",
       "      'direction': 1.0,\n",
       "      'focus_point': 'end'},\n",
       "     {'scaling_factor': 2.2004481380470184,\n",
       "      'direction': 1.0,\n",
       "      'focus_point': 'end'},\n",
       "     {'scaling_factor': 2.267693617985819,\n",
       "      'direction': 1.0,\n",
       "      'focus_point': 'end'}],\n",
       "    4]),\n",
       "  (<pygad.pygad.GA at 0x28d547f10>,\n",
       "   [[0.5388386523633439, 0.09824468871057035],\n",
       "    [{'scaling_factor': 4.148079077799078,\n",
       "      'direction': 2.0,\n",
       "      'focus_point': 'start'},\n",
       "     {'scaling_factor': 1.2500836039699728,\n",
       "      'direction': 2.0,\n",
       "      'focus_point': 'start'},\n",
       "     {'scaling_factor': 1.532866356110212,\n",
       "      'direction': 2.0,\n",
       "      'focus_point': 'start'},\n",
       "     {'scaling_factor': 1.7845702284567126,\n",
       "      'direction': 2.0,\n",
       "      'focus_point': 'start'}],\n",
       "    4]),\n",
       "  (<pygad.pygad.GA at 0x3075f8290>,\n",
       "   [[0.5393936130951351, 0.09824468871057035],\n",
       "    [{'scaling_factor': 4.368494893103088,\n",
       "      'direction': 0.0,\n",
       "      'focus_point': 'middle'},\n",
       "     {'scaling_factor': 4.083066638618944,\n",
       "      'direction': 0.0,\n",
       "      'focus_point': 'middle'},\n",
       "     {'scaling_factor': 2.9169394953124694,\n",
       "      'direction': 0.0,\n",
       "      'focus_point': 'middle'},\n",
       "     {'scaling_factor': 1.2935539112950991,\n",
       "      'direction': 0.0,\n",
       "      'focus_point': 'middle'}],\n",
       "    4])],\n",
       " [(<pygad.pygad.GA at 0x3a7208810>,\n",
       "   [[0.19174500921321036, -0.01945363033378955],\n",
       "    [{'scaling_factor': 1.3203392238679725,\n",
       "      'direction': 1.0,\n",
       "      'focus_point': 'end'},\n",
       "     {'scaling_factor': 0.48368143948447073,\n",
       "      'direction': 1.0,\n",
       "      'focus_point': 'end'},\n",
       "     {'scaling_factor': 2.8422658395189755,\n",
       "      'direction': 1.0,\n",
       "      'focus_point': 'end'},\n",
       "     {'scaling_factor': 0.4486054086482246,\n",
       "      'direction': 1.0,\n",
       "      'focus_point': 'end'}],\n",
       "    2]),\n",
       "  (<pygad.pygad.GA at 0x4d28c9350>,\n",
       "   [[0.2003132281674589, -0.019588170157593066],\n",
       "    [{'scaling_factor': 3.694047433718923,\n",
       "      'direction': 2.0,\n",
       "      'focus_point': 'middle'},\n",
       "     {'scaling_factor': 3.7344674940356897,\n",
       "      'direction': 2.0,\n",
       "      'focus_point': 'middle'},\n",
       "     {'scaling_factor': 1.6120049380809482,\n",
       "      'direction': 2.0,\n",
       "      'focus_point': 'middle'},\n",
       "     {'scaling_factor': 1.8428631027654911,\n",
       "      'direction': 2.0,\n",
       "      'focus_point': 'middle'}],\n",
       "    4]),\n",
       "  (<pygad.pygad.GA at 0x288657a50>,\n",
       "   [[0.2042660901203206, -0.031229607811703364],\n",
       "    [{'scaling_factor': 1.8220644998049624,\n",
       "      'direction': 2.0,\n",
       "      'focus_point': 'start'},\n",
       "     {'scaling_factor': 4.807464339347786,\n",
       "      'direction': 2.0,\n",
       "      'focus_point': 'start'},\n",
       "     {'scaling_factor': 2.9415794037317333,\n",
       "      'direction': 2.0,\n",
       "      'focus_point': 'start'},\n",
       "     {'scaling_factor': 2.106497666979862,\n",
       "      'direction': 2.0,\n",
       "      'focus_point': 'start'}],\n",
       "    4]),\n",
       "  (<pygad.pygad.GA at 0x335fe9a50>,\n",
       "   [[0.2042660901203206, -0.031229607811703364],\n",
       "    [{'scaling_factor': 1.8220644998049624,\n",
       "      'direction': 2.0,\n",
       "      'focus_point': 'start'},\n",
       "     {'scaling_factor': 4.807464339347786,\n",
       "      'direction': 2.0,\n",
       "      'focus_point': 'start'},\n",
       "     {'scaling_factor': 2.9415794037317333,\n",
       "      'direction': 2.0,\n",
       "      'focus_point': 'start'},\n",
       "     {'scaling_factor': 2.106497666979862,\n",
       "      'direction': 2.0,\n",
       "      'focus_point': 'start'}],\n",
       "    4]),\n",
       "  (<pygad.pygad.GA at 0x7dd816b10>,\n",
       "   [[0.2042660901203206, -0.031229607811703364],\n",
       "    [{'scaling_factor': 1.8220644998049624,\n",
       "      'direction': 2.0,\n",
       "      'focus_point': 'start'},\n",
       "     {'scaling_factor': 4.807464339347786,\n",
       "      'direction': 2.0,\n",
       "      'focus_point': 'start'},\n",
       "     {'scaling_factor': 2.9415794037317333,\n",
       "      'direction': 2.0,\n",
       "      'focus_point': 'start'},\n",
       "     {'scaling_factor': 2.106497666979862,\n",
       "      'direction': 2.0,\n",
       "      'focus_point': 'start'}],\n",
       "    4])]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyberaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
