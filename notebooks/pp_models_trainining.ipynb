{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ISOLTATION FOREST - FITNESS FUNCTION\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"../data/processed/CTU13_Normal_Traffic_20.csv\")  # Replace with your file's path\n",
    "\n",
    "# If you have labels and features separated, adjust accordingly. \n",
    "# Isolation Forest often works in an unsupervised manner without labels.\n",
    "X = df  # You may select specific columns as features if needed\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Isolation Forest model\n",
    "iso_forest = IsolationForest(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "iso_forest.fit(X_train)\n",
    "\n",
    "# Predict on the test data\n",
    "# Values of -1 represent anomalies, 1 represents normal observations\n",
    "predictions = iso_forest.predict(X_test)\n",
    "\n",
    "# TODO: potem zrobic to samo juz bez splita - tylko dla bajery przetestowac na razie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1  1 ...  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 1/34 [..............................] - ETA: 4s - loss: 76766248960000.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 02:14:08.856512: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 3ms/step - loss: 84304243720192.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 2/50\n",
      "34/34 [==============================] - 0s 930us/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 0s 926us/step - loss: 84304243720192.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 0s 918us/step - loss: 84304252108800.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 0s 993us/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 84304226942976.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 84304252108800.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 84304243720192.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 84304243720192.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 84304243720192.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 84304226942976.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 84304243720192.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 18/50\n",
      "34/34 [==============================] - 0s 925us/step - loss: 84304243720192.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 19/50\n",
      "34/34 [==============================] - 0s 941us/step - loss: 84304243720192.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 20/50\n",
      "34/34 [==============================] - 0s 946us/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 21/50\n",
      "34/34 [==============================] - 0s 985us/step - loss: 84304252108800.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 22/50\n",
      "34/34 [==============================] - 0s 996us/step - loss: 84304243720192.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 23/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 84304252108800.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 24/50\n",
      "34/34 [==============================] - 0s 903us/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 25/50\n",
      "34/34 [==============================] - 0s 917us/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 26/50\n",
      "34/34 [==============================] - 0s 941us/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 27/50\n",
      "34/34 [==============================] - 0s 921us/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 28/50\n",
      "34/34 [==============================] - 0s 921us/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 29/50\n",
      "34/34 [==============================] - 0s 990us/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 30/50\n",
      "34/34 [==============================] - 0s 932us/step - loss: 84304243720192.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 31/50\n",
      "34/34 [==============================] - 0s 916us/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 32/50\n",
      "34/34 [==============================] - 0s 934us/step - loss: 84304243720192.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 33/50\n",
      "34/34 [==============================] - 0s 868us/step - loss: 84304243720192.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 34/50\n",
      "34/34 [==============================] - 0s 949us/step - loss: 84304243720192.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 35/50\n",
      "34/34 [==============================] - 0s 908us/step - loss: 84304243720192.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 36/50\n",
      "34/34 [==============================] - 0s 939us/step - loss: 84304243720192.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 37/50\n",
      "34/34 [==============================] - 0s 913us/step - loss: 84304252108800.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 38/50\n",
      "34/34 [==============================] - 0s 977us/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 39/50\n",
      "34/34 [==============================] - 0s 989us/step - loss: 84304252108800.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 40/50\n",
      "34/34 [==============================] - 0s 927us/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 41/50\n",
      "34/34 [==============================] - 0s 966us/step - loss: 84304260497408.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 42/50\n",
      "34/34 [==============================] - 0s 951us/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 43/50\n",
      "34/34 [==============================] - 0s 933us/step - loss: 84304243720192.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 44/50\n",
      "34/34 [==============================] - 0s 950us/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 45/50\n",
      "34/34 [==============================] - 0s 971us/step - loss: 84304243720192.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 46/50\n",
      "34/34 [==============================] - 0s 942us/step - loss: 84304252108800.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 47/50\n",
      "34/34 [==============================] - 0s 973us/step - loss: 84304243720192.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 48/50\n",
      "34/34 [==============================] - 0s 910us/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 49/50\n",
      "34/34 [==============================] - 0s 925us/step - loss: 84304235331584.0000 - val_loss: 84961273053184.0000\n",
      "Epoch 50/50\n",
      "34/34 [==============================] - 0s 935us/step - loss: 84304252108800.0000 - val_loss: 84961273053184.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x291d05650>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# AUTO ENCODER - ATTACKED MODEL\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import pandas as pd\n",
    "\n",
    "# Import necessary libraries\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"../data/processed/CTU13_Normal_Traffic_20.csv\") \n",
    "# Prepare the data (example without separating into training and testing sets)\n",
    "# You might need to split and scale the data beforehand\n",
    "X = df.values  # Assuming 'df' is your DataFrame\n",
    "\n",
    "# Define the autoencoder structure\n",
    "input_dim = X.shape[1]  # Number of features\n",
    "encoding_dim = 32  # This can be adjusted according to your needs\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "# Encoding layer\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "# Decoding layer\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "# Build the autoencoder model\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X, X,  # Y is the same as X because we try to reconstruct the input\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622/622 [==============================] - 0s 241us/step\n",
      "334/334 [==============================] - 0s 240us/step\n"
     ]
    }
   ],
   "source": [
    "from src.data.cic_preprocess import preprocess_traffic\n",
    "\n",
    "df2 = preprocess_traffic('../data/interim/xflow.csv')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "X_anomalies = df2.values  # Przyjmujemy, że df2 to DataFrame z anomaliami\n",
    "\n",
    "# Skalowanie danych (jeśli wcześniej skalowałeś dane treningowe)\n",
    "scaler = StandardScaler().fit(X)  # Użyj tych samych parametrów skalowania co dla danych treningowych\n",
    "X_anomalies_scaled = scaler.transform(X_anomalies)\n",
    "\n",
    "# Użyj autoenkodera do rekonstrukcji danych zawierających anomalie\n",
    "reconstructed = autoencoder.predict(X_anomalies_scaled)\n",
    "\n",
    "# Oblicz błąd rekonstrukcji jako różnicę między danymi wejściowymi a rekonstrukcją\n",
    "reconstruction_error = np.mean(np.power(X_anomalies_scaled - reconstructed, 2), axis=1)\n",
    "\n",
    "# Możesz następnie ustalić próg, powyżej którego obserwacje będą uznawane za anomalie\n",
    "# Prog można ustalić na podstawie błędu rekonstrukcji na danych treningowych (normalnych)\n",
    "reconstruction_error_normal = np.mean(np.power(X - autoencoder.predict(X), 2), axis=1)\n",
    "threshold = np.percentile(reconstruction_error_normal, 10)  # np. 95 percentyl błędu jako próg\n",
    "\n",
    "# Klasyfikacja na podstawie progu\n",
    "anomalies_predicted = reconstruction_error > threshold\n",
    "\n",
    "# Teraz `anomalies_predicted` zawiera True dla przewidzianych anomalii i False dla normalnych obserwacji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(anomalies_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38898, 32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mal = pd.read_csv(\"../data/processed/CTU13_attack_all_scenarios.csv\")\n",
    "df_mal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42651, 32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_norm = pd.read_csv(\"../data/processed/CTU13_Normal_Traffic_80.csv\")\n",
    "df_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "df_norm['label'] = 0\n",
    "df_mal['label'] = 1\n",
    "\n",
    "df_combined = pd.concat([df_norm, df_mal], ignore_index=True)\n",
    "\n",
    "X = df_combined.drop('label', axis=1)\n",
    "y = df_combined['label']\n",
    "\n",
    "model = CatBoostClassifier(verbose=0)  # `verbose=0` wyłącza logowanie do konsoli dla czytelności\n",
    "model.fit(X, y)\n",
    "\n",
    "df_to_predict = preprocess_traffic('../data/interim/xflow.csv')\n",
    "predictions = model.predict(df_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9992961287078934"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predictions)/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jak sobie radzi z normalnymi\n",
    "\n",
    "df_to_predict = pd.read_csv(\"../data/processed/CTU13_Normal_Traffic_20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(df_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004126418456344368"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predictions)/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: w wersji 100% bardzo skuteczne przewidywanie, sprawdzić przewidywanie po wyjęciu odpowiedniego sceanriusza:) - powinno być wystarczająco - należy zautomatyzować taki pipeline do testowania\n",
    "\n",
    "# 1) końcowe modele i pipeline do szybkiego testowania wszystkiego []\n",
    "# 2) techniki perturbacji []\n",
    "# 3) funkcja fitness;) - prawdop błąd śr. kw. []\n",
    "# 4) spięcie wszystkiego w ten algorym i oglądanie parametrów/wyniki/interpretacja itp. []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyberaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
